{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LDV data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.spatial import Delaunay\n",
    "import math\n",
    "\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from vtk import *\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import display\n",
    "import timeit\n",
    "from icecream import ic\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "%matplotlib --list\n",
    "%matplotlib inline\n",
    "plt.isinteractive()\n",
    "#plt.rc('font', family='sans-serif')\n",
    "#plt.rc('text', usetex=True)\n",
    "#plt.rcParams['text.latex.preamble'] = [\n",
    "#       r'\\usepackage{siunitx}',   # i need upright \\micro symbols, but you need...\n",
    "#       r'\\sisetup{detect-all}',   # ...this to force siunitx to actually use your fonts\n",
    "#       r'\\usepackage{helvet}',    # set the normal font here\n",
    "#       r'\\usepackage{sansmath}',  # load up the sansmath so that math -> helvet\n",
    "#       r'\\sansmath'               # <- tricky! -- gotta actually tell tex to use!\n",
    "#] \n",
    "\n",
    "pd.set_option('display.float_format','{:.6e}'.format)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('colheader_justify', 'right')\n",
    "\n",
    "def Display(data):\n",
    "    \"\"\"\n",
    "    Displays the entire DataFrame in a Jupyter Notebook.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The DataFrame to be displayed. If the number of rows is less than 100, \n",
    "                         all rows will be shown. All columns will be shown regardless of the number of columns.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    pd.set_option('display.width', 1000)\n",
    "    if len(data)<100:\n",
    "        pd.set_option(\"display.max_rows\",len(data))\n",
    "    pd.set_option('display.max_columns',len(data.columns))\n",
    "    display(data)\n",
    "    pd.reset_option(\"display.max_rows\")\n",
    "    pd.reset_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.isinteractive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    \"\"\"\n",
    "    Custom exception class to stop the execution of a Jupyter notebook cell.\n",
    "\n",
    "    This exception can be raised to halt the execution of the current cell\n",
    "    without displaying a traceback.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    _render_traceback_():\n",
    "        Overrides the default traceback rendering to suppress the traceback output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit():\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myround(x,margin,base=5):\n",
    "    \"\"\"\n",
    "    Rounds a number to the nearest multiple of a specified base, adjusted by a margin.\n",
    "\n",
    "    Parameters:\n",
    "    x (float): The number to be rounded.\n",
    "    margin (float): The margin to adjust the rounding direction.\n",
    "    base (int, optional): The base to which the number should be rounded. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    float: The rounded number.\n",
    "    \"\"\"\n",
    "        \n",
    "    x=x*10+np.sign(margin)*base\n",
    "    return base*round(x/base)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check(outfolder):\n",
    "    \"\"\"\n",
    "    Checks the existence and contents of a specified folder.\n",
    "    If the folder does not exist, it creates the folder and returns True.\n",
    "    If the folder exists, it checks for files ending with 'fth'. If there are fewer than 6 such files, it returns True.\n",
    "    Otherwise, it returns False.\n",
    "    Args:\n",
    "        outfolder (Path): The path to the folder to check.\n",
    "    Returns:\n",
    "        bool: True if the folder was created or contains fewer than 6 'fth' files, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if not outfolder.exists():\n",
    "        outfolder.mkdir(parents=True,exist_ok=True)\n",
    "        return(True)\n",
    "    else:\n",
    "        items=[item for item in outfolder.iterdir() if item.is_file() and item.name.endswith('fth')]\n",
    "        \n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init_LDV():\n",
    "    \n",
    "    Settings=pd.DataFrame(index=['Root', 'PropModel', 'Case', \n",
    "                            'AxisScaleFactor', 'Rref', 'ExternalChannels','nStd', 'Period', 'Step', 'Wleft', 'Wright',\n",
    "                            'VerticalUpPhaseOffset', 'VerticalDownPhaseOffset', \n",
    "                            'HorizontalLeftPhaseOffset', 'HorizontalRightPhaseOffset',\n",
    "                            'RefractiveIndexCorrection','Interpolation',\n",
    "                            'RadiusRange','PlaneRange',\n",
    "                            'GenerateDatabase', 'PhaseAnalysis', \n",
    "                            'GeneratePolarPlots',\n",
    "                            'Autoscale','PolarPlotRadiusLimits',\n",
    "                            'Va_samp_range', 'Va_mean_range', 'Va_sdev_range', \n",
    "                            'Vr_samp_range', 'Vr_mean_range', 'Vr_sdev_range',\n",
    "                            'Vt_samp_range', 'Vt_mean_range', 'Vt_sdev_range',\n",
    "                            'Verbose','Overwrite',\n",
    "                            'ShowPhasePlots','ShowPolarPlots'],\n",
    "                     columns=['Value', 'Comment'])\n",
    "\n",
    "    Settings.at['Root','Value'] = '.'\n",
    "    Settings.at['Root','Comment'] = 'Root folder'\n",
    "    Settings.at['PropModel','Value'] = 'CRP'\n",
    "    Settings.at['PropModel','Comment'] = 'Name of propeller model'\n",
    "    Settings.at['Case','Value'] = 'J140S800'\n",
    "    Settings.at['Case','Comment'] = 'Case'\n",
    "   \n",
    "    Settings.at['Rref','Value'] = 100.000\n",
    "    Settings.at['Rref','Comment'] = 'Reference radius in mm'\n",
    "    Settings.at['ExternalChannels','Value'] = []\n",
    "    Settings.at['ExternalChannels','Comment'] = 'List of external channels'\n",
    "    Settings.at['AxisScaleFactor','Value'] = [1, 1.33, 1.] \n",
    "    Settings.at['AxisScaleFactor','Comment'] = 'Axis scale factor [X,Y,Z]'\n",
    "    \n",
    "    Settings.at['nStd','Value'] = 4\n",
    "    Settings.at['nStd','Comment'] = 'Number of std to remove spurious data'\n",
    "    Settings.at['Period','Value'] = 360.0\n",
    "    Settings.at['Period','Comment'] = 'Modulo'\n",
    "    Settings.at['Step','Value'] = 2.0\n",
    "    Settings.at['Step','Comment'] = 'Step between slots'\n",
    "    Settings.at['Wleft','Value'] = 1.0\n",
    "    Settings.at['Wleft','Comment'] = 'Slot width to the left'\n",
    "    Settings.at['Wright','Value'] = 1.0\n",
    "    Settings.at['Wright','Comment'] = 'Slot width to the right'\n",
    "    Settings.at['VerticalUpPhaseOffset','Value'] = 0.0\n",
    "    Settings.at['VerticalUpPhaseOffset','Comment'] = 'Phase offset for vertical up axis'\n",
    "    Settings.at['VerticalDownPhaseOffset','Value'] = 0.0\n",
    "    Settings.at['VerticalDownPhaseOffset','Comment'] = 'Phase offset for vertical down axis'\n",
    "    Settings.at['HorizontalLeftPhaseOffset','Value'] = 0.0\n",
    "    Settings.at['HorizontalLeftPhaseOffset','Comment'] = 'Phase offset for horizontal left axis'\n",
    "    Settings.at['HorizontalRightPhaseOffset','Value'] = 0.0\n",
    "    Settings.at['HorizontalRightPhaseOffset','Comment'] = 'Phase offset for horizontal right axis'\n",
    "    Settings.at['RefractiveIndexCorrection','Value'] = 1.0\n",
    "    Settings.at['RefractiveIndexCorrection','Comment'] = 'Refractive index correction (n=1.33 for 1.0)'\n",
    "    Settings.at['Interpolation','Value'] = 'linear'\n",
    "    Settings.at['Interpolation','Comment'] = 'linear, thin_plate_spline, cubic, quintic, gaussian'\n",
    "    \n",
    "    Settings.at['RadiusRange','Value'] = [0,1.0]\n",
    "    Settings.at['RadiusRange','Comment'] = 'Radius range for analysis (in mm)'\n",
    "    Settings.at['PlaneRange','Value'] = [0,1,2]\n",
    "    Settings.at['PlaneRange','Comment'] = 'Plane range for analysis (-1 for all planes)'\n",
    "    Settings.at['GenerateDatabase','Value'] = True\n",
    "    Settings.at['GenerateDatabase','Comment'] = 'True to generate database'\n",
    "    Settings.at['PhaseAnalysis','Value'] = True\n",
    "    Settings.at['PhaseAnalysis','Comment'] = 'True to run phase analysis'\n",
    "\n",
    "    Settings.at['GeneratePolarPlots','Value'] = True\n",
    "    Settings.at['GeneratePolarPlots','Comment'] = 'True to generate database'\n",
    "    Settings.at['Autoscale','Value'] = True\n",
    "    Settings.at['Autoscale','Comment'] = 'Set autoscale for polar plots'\n",
    "    Settings.at['PolarPlotRadiusLimits','Value'] = True\n",
    "    Settings.at['PolarPlotRadiusLimits','Comment'] = 'Radius limits for polar plots'\n",
    "    \n",
    "    Settings.at['Va_samp_range','Value'] = [0,4000]\n",
    "    Settings.at['Va_samp_range','Comment'] = 'Polar plot range (Va samples)'\n",
    "    Settings.at['Va_mean_range','Value'] = [5,15]\n",
    "    Settings.at['Va_mean_range','Comment'] = 'Polar plot range (Va mean)'\n",
    "    Settings.at['Va_sdev_range','Value'] = [0,2.0]\n",
    "    Settings.at['Va_sdev_range','Comment'] = 'Polar plot range (Va rms)'\n",
    "    \n",
    "    Settings.at['Vr_samp_range','Value'] = [0,4000]\n",
    "    Settings.at['Vr_samp_range','Comment'] = 'Polar plot range (Vr samples)'\n",
    "    Settings.at['Vr_mean_range','Value'] = [-1.5,1.5]\n",
    "    Settings.at['Vr_mean_range','Comment'] = 'Polar plot range (Vr mean)'\n",
    "    Settings.at['Vr_sdev_range','Value'] = [0,2.0]\n",
    "    Settings.at['Vr_sdev_range','Comment'] = 'Polar plot range (Vr rms)'\n",
    "\n",
    "    Settings.at['Vt_samp_range','Value'] = [0,4000]\n",
    "    Settings.at['Vt_samp_range','Comment'] = 'Polar plot range (Vt samples)'\n",
    "    Settings.at['Vt_mean_range','Value'] = [-1.5,1.5]\n",
    "    Settings.at['Vt_mean_range','Comment'] = 'Polar plot range (Vt mean)'\n",
    "    Settings.at['Vt_sdev_range','Value'] = [0,2.0]\n",
    "    Settings.at['Vt_sdev_range','Comment'] = 'Polar plot range (Vt rms)'\n",
    "    \n",
    "    Settings.at['Verbose','Value'] = True\n",
    "    Settings.at['Verbose','Comment'] = 'Verbose output'\n",
    "    Settings.at['Overwrite','Value'] = True\n",
    "    Settings.at['Overwrite','Comment'] = 'Overwrite existing files'\n",
    "    Settings.at['ShowPhasePlots','Value'] = True\n",
    "    Settings.at['ShowPhasePlots','Comment'] = 'Show phase plots'\n",
    "    Settings.at['ShowPolarPlots','Value'] = True\n",
    "    Settings.at['ShowPolarPlots','Comment'] = 'Show polar plots'\n",
    "    \n",
    "    Settings.index.name='Parameter'\n",
    "\n",
    "    return(Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GetSettings(f):\n",
    "    \"\"\"\n",
    "    Retrieves or initializes settings for LDV (Laser Doppler Velocimetry) analysis.\n",
    "    If the specified file does not exist, it initializes the settings by calling Init_LDV(),\n",
    "    creates the necessary directories, and saves the settings to a CSV file. If the file exists,\n",
    "    it loads the settings from the CSV file, processes them to remove comments and duplicates,\n",
    "    and cleans up the data.\n",
    "    Args:\n",
    "        f (pathlib.Path): The file path where the settings are stored or will be stored.\n",
    "    Returns:\n",
    "        pandas.DataFrame: The settings DataFrame with 'Value' and 'Comment' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    Settings = Init_LDV()\n",
    "\n",
    "    if not f.exists():\n",
    "        print(f'Creating {f}')\n",
    "        f.parent.mkdir(parents=True, exist_ok=True)\n",
    "        Settings.to_csv(f,header=['Value','Comment'],sep=',')\n",
    "    else:\n",
    "        # Load the settings\n",
    "        Settings = pd.read_csv('SettingsLDV.csv',delimiter=',',\n",
    "                               index_col=0,na_filter=False)\n",
    "        Settings = Settings[~Settings.index.str.startswith('#')]\n",
    "        Settings = Settings[~Settings.index.duplicated(keep='last')]\n",
    "        \n",
    "        #def split_name(name):\n",
    "        #    return pd.Series(name.split(\"#\", 1))\n",
    "        #Settings[['Value', 'Comment']] = Settings['Value'].apply(split_name)\n",
    "        \n",
    "        Settings = Settings.replace(np.nan, '-', regex=True)\n",
    "        Settings.index = [x.lstrip().rstrip() for x in Settings.index]\n",
    "        for col in ['Value','Comment']:\n",
    "            Settings[col] = Settings[col].apply(lambda x: str(x).lstrip().rstrip().strip(\"'\"))\n",
    "    \n",
    "    return(Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunSettings():\n",
    "    \"\"\"\n",
    "    Configures and initializes settings for LDV analysis from a CSV file.\n",
    "    This function reads settings from a CSV file named 'SettingsLDV.csv' located in the current directory.\n",
    "    It processes and converts the settings into appropriate data types, and then stores them in global variables\n",
    "    for further use in the analysis.\n",
    "    Args:\n",
    "        verbose (bool, optional): If True, displays the settings and their types. Defaults to False.\n",
    "    Global Variables:\n",
    "        settings (dict): A dictionary containing the processed settings.\n",
    "        DataFolder (Path): Path object pointing to the data folder based on the settings.\n",
    "        OutFolder (Path): Path object pointing to the output folder based on the settings.\n",
    "        Overlap (float): The overlap percentage between adjacent slots.\n",
    "    Notes:\n",
    "        - The function expects the CSV file to have specific columns and values.\n",
    "        - The settings are categorized and processed based on their expected data types.\n",
    "        - The function also calculates additional settings like 'Wslot' and 'IntervalClosed'.\n",
    "        - If 'verbose' is True, the settings and their types are displayed for debugging purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    Settings=GetSettings(Path('./SettingsLDV.csv'))\n",
    "\n",
    "    Values = ['Root', 'PropModel', 'Case', 'Interpolation']\n",
    "\n",
    "    Values = ['Rref', 'Period', 'Step', 'Wleft', 'Wright', \n",
    "              'VerticalUpPhaseOffset', 'VerticalDownPhaseOffset', \n",
    "              'HorizontalLeftPhaseOffset', 'HorizontalRightPhaseOffset',\n",
    "              'RefractiveIndexCorrection']\n",
    "    for value in Values:\n",
    "        Settings.at[value,'Value'] = float(Settings.at[value,'Value'])\n",
    "\n",
    "    Values = ['nStd']\n",
    "    for value in Values:\n",
    "        Settings.at[value,'Value'] = int(Settings.at[value,'Value'])\n",
    "\n",
    "    Values = ['GenerateDatabase', 'PhaseAnalysis', 'GeneratePolarPlots', 'Autoscale', \n",
    "              'Verbose', 'Overwrite', \n",
    "              'ShowPhasePlots', 'ShowPolarPlots']\n",
    "    for value in Values:\n",
    "        Settings.at[value,'Value'] = True if Settings.at[value,'Value']=='True' else False\n",
    "\n",
    "    Values = ['AxisScaleFactor',\n",
    "              'RadiusRange','PolarPlotRadiusLimits',\n",
    "              'Va_samp_range', 'Va_mean_range', 'Va_sdev_range', \n",
    "              'Vr_samp_range', 'Vr_mean_range', 'Vr_sdev_range',\n",
    "              'Vt_samp_range', 'Vt_mean_range', 'Vt_sdev_range']\n",
    "    for value in Values:\n",
    "        lst = Settings.at[value,'Value']\n",
    "        lst = list(lst.strip('[]').split(','))\n",
    "        Settings.at[value,'Value']= [float(a) for a in lst]\n",
    "\n",
    "    Values = ['ExternalChannels','PlaneRange']\n",
    "    for value in Values:\n",
    "        lst = Settings.at[value,'Value']\n",
    "        lst = list(lst.strip('[]').split(','))\n",
    "        Settings.at[value,'Value']= [int(a) for a in lst]\n",
    "\n",
    "    global settings\n",
    "    \n",
    "    settings = Settings['Value'].to_dict()\n",
    "    settings['Wslot'] = settings['Wleft']+settings['Wright']\n",
    "    settings['IntervalClosed'] = 'left'\n",
    "\n",
    "    settings['DataFolder'] = Path(settings['Root'],'Data/Exports/%s_LDV/%s' % \\\n",
    "        (settings['PropModel'],settings['Case']))\n",
    "    settings['OutFolder'] = Path(settings['Root'],'Analysis/%s_LDV/%s' % (settings['PropModel'],settings['Case']))\n",
    "\n",
    "    if settings['Verbose']:\n",
    "        display(Settings)\n",
    "        display(Settings['Value'].apply(type))\n",
    "        display(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunSettings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveCSV(folder,file,data,verbose=False):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file in the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    folder (Path): The directory where the CSV file will be saved. If it does not exist, it will be created.\n",
    "    file (str): The name of the CSV file (without the .csv extension).\n",
    "    data (DataFrame): The data to be saved to the CSV file.\n",
    "    verbose (bool, optional): If True, the path of the saved file will be displayed. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    folder.mkdir(parents=True,exist_ok=True)\n",
    "    dataout=Path(folder,file+'.csv')\n",
    "    if verbose:\n",
    "        display(dataout)\n",
    "    data.to_csv(dataout)\n",
    "\n",
    "def SaveXLS(folder,file,data,verbose=False):\n",
    "    \"\"\"\n",
    "    Save the given data to an Excel file in the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    folder (Path): The directory where the Excel file will be saved.\n",
    "    file (str): The name of the Excel file (without extension).\n",
    "    data (DataFrame): The data to be saved in the Excel file.\n",
    "    verbose (bool, optional): If True, display the path of the saved file. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    folder.mkdir(parents=True,exist_ok=True)\n",
    "    dataout=Path(folder,file+'.xlsx')\n",
    "    if verbose:\n",
    "        display(dataout)\n",
    "    writer = pd.ExcelWriter(dataout, engine='openpyxl')\n",
    "    data.to_excel(writer, sheet_name=('Stats'))\n",
    "    writer.close()\n",
    "\n",
    "def SaveFTH(folder,file,data,verbose=False):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a Feather file with LZ4 compression.\n",
    "\n",
    "    Parameters:\n",
    "    folder (Path): The directory where the Feather file will be saved.\n",
    "    file (str): The name of the Feather file (without extension).\n",
    "    data (DataFrame): The DataFrame to be saved.\n",
    "    verbose (bool, optional): If True, display the path of the saved file. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    folder.mkdir(parents=True,exist_ok=True)\n",
    "    dataout=Path(folder,file+'.fth')\n",
    "    if verbose:\n",
    "        display(dataout)\n",
    "    data.to_feather(dataout,compression='lz4')\n",
    "\n",
    "def SaveMAT(folder,file,data,verbose=False):\n",
    "    \"\"\"\n",
    "    Save data to a .mat file in the specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    folder (Path): The directory where the .mat file will be saved.\n",
    "    file (str): The name of the .mat file (without extension).\n",
    "    data (DataFrame): The data to be saved, which will be converted to a dictionary of lists.\n",
    "    verbose (bool, optional): If True, display the path of the saved file. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    folder.mkdir(parents=True,exist_ok=True)\n",
    "    dataout=Path(folder,file+'.mat')\n",
    "    if verbose:\n",
    "        display(dataout)\n",
    "    mydict = data.to_dict('list')\n",
    "    io.savemat(str(dataout), {'structs':mydict})\n",
    "\n",
    "def LoadFTH(folder, file):\n",
    "   \n",
    "    v = pd.DataFrame()\n",
    "    f=Path(folder,file+'.fth')\n",
    "    if f.exists():\n",
    "        v=pd.read_feather(f)\n",
    "\n",
    "    return (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadStatFiles(datafolder, case, verbose=False):\n",
    "    \"\"\"\n",
    "    Reads statistical files from a specified data folder and case, and returns a concatenated DataFrame.\n",
    "    Parameters:\n",
    "    datafolder (Path): The path to the folder containing the data.\n",
    "    case (str): The specific case to read data for.\n",
    "    verbose (bool, optional): If True, prints and displays additional information for debugging. Default is False.\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the concatenated data from all blocks.\n",
    "    None: If the number of files in a block does not match the number of entries in the data.\n",
    "    Notes:\n",
    "    - The function expects the data files to be in CSV format.\n",
    "    - The function assumes that each block folder contains a CSV file named after the block.\n",
    "    - The function also assumes that each block folder contains a subfolder named 'CH1' with files whose names end in a 6-digit number.\n",
    "    - The function will print and display additional information if verbose is set to True.\n",
    "    \"\"\"\n",
    "\n",
    "    Data=pd.DataFrame()\n",
    "\n",
    "    if verbose:\n",
    "        print(datafolder,case)\n",
    "    Block=[item for item in datafolder.iterdir() if item.is_dir()]\n",
    "    if len(Block)==0:\n",
    "        print('No blocks found')\n",
    "        return(Data)\n",
    "    Block=sorted(Block, key=lambda x: x.name)\n",
    "    if verbose:\n",
    "        display(Block)\n",
    "    for block in Block[:]:\n",
    "        file=Path(block,block.name+'.csv')\n",
    "        if not file.exists():\n",
    "            print('File %s not found' % file)\n",
    "            continue\n",
    "        if verbose:\n",
    "            display(file)\n",
    "        if file.exists():\n",
    "            #filenames=pd.read_csv(file,sep=',',skipinitialspace=True,skiprows=lambda x: x % 3,names=['File'])\n",
    "            #header=pd.read_csv(file,sep=',',skipinitialspace=True,skiprows=1,nrows=1,header=None).values[0]\n",
    "            #display(filenames)    \n",
    "            #display(header) \n",
    "            \n",
    "            data=pd.read_csv(file,sep=',',skipinitialspace=True)\n",
    "            data.columns=[col.strip('\\\"') for col in data.columns]\n",
    "            data.columns=[col.strip() for col in data.columns]\n",
    "            #data=pd.concat([data,filenames],axis=1)\n",
    "            data['Block']=block.name\n",
    "            Files=[item.stem for item in Path(block,'CH1').iterdir() if item.is_file()]\n",
    "            Files.sort(key=lambda x : int(x[-6:]))\n",
    "            if verbose:\n",
    "                ic(len(Files))\n",
    "                ic(len(data))\n",
    "            if len(Files)!=len(data):\n",
    "                print('Number of files in block does not match number of files in data')\n",
    "                #return(None,None)\n",
    "            data['File']=Files\n",
    "            data['Point']=data['Sequence Number'].apply(lambda x: int(x))\n",
    "            Data=pd.concat([Data,data])\n",
    "\n",
    "    if not len(Data):\n",
    "        return(Data)\n",
    "    Data.sort_values(by=['Block','Point'],inplace=True,ascending=True,ignore_index=True)\n",
    "    #Data.sort_values(by=['X Axis Position','Y Axis Position','Z Axis Position'],\n",
    "    #inplace=True,ascending=True,ignore_index=True)\n",
    "\n",
    "    if verbose:\n",
    "        Display(Data)\n",
    "        display(Data.info())\n",
    "\n",
    "    return (Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LoadStatFiles(verbose=False):\n",
    "    \"\"\"\n",
    "    Load statistical files, process the data, and save the results.\n",
    "    This function reads statistical data files from a specified folder, processes the data by changing axis orientation and scaling, \n",
    "    and then saves the processed data to output files. Optionally, it can display the first few rows of the processed data.\n",
    "    Args:\n",
    "        verbose (bool, optional): If True, displays the first few rows of the processed data. Defaults to False.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    Data=ReadStatFiles(settings['DataFolder'],settings['Case'],verbose=True)\n",
    "    if not len(Data):\n",
    "        print('No Data found in %s' % settings['DataFolder'])\n",
    "        exit()\n",
    "    Block = Data['Block'].drop_duplicates().to_list()\n",
    "    Data=Data.reset_index()\n",
    "    \n",
    "    # Change axis orientation and scaling\n",
    "    Data['X (mm)']=Data['X Axis Position'].apply(lambda x: x*settings['AxisScaleFactor'][0])\n",
    "    Data['Y (mm)']=Data['Y Axis Position'].apply(lambda x: x*settings['AxisScaleFactor'][1])\n",
    "    Data['Z (mm)']=Data['Z Axis Position'].apply(lambda x: x*settings['AxisScaleFactor'][2])\n",
    "    Data['R (mm)']=Data['Y (mm)']**2+Data['Z (mm)']**2\n",
    "    Data['R (mm)']=Data['R (mm)'].apply(lambda x: np.sqrt(x))\n",
    "\n",
    "    SaveXLS(settings['OutFolder'],'%s_Stats0' % settings['Case'],Data)\n",
    "    SaveFTH(settings['OutFolder'],'%s_Stats0' % settings['Case'],Data)\n",
    "\n",
    "    if verbose:\n",
    "        display(Data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize into planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindPlanes(verbose=False):\n",
    "    \"\"\"\n",
    "    Find and categorize planes from measurement data.\n",
    "    This function reads measurement data, performs hierarchical clustering to identify planes,\n",
    "    and visualizes the results in a 3D scatter plot. The identified planes are saved to files.\n",
    "    Parameters:\n",
    "    verbose (bool): If True, displays intermediate data and plots. Default is False.\n",
    "    Returns:\n",
    "    None\n",
    "    Notes:\n",
    "    - The function reads data from a feather file specified by the 'settings' dictionary.\n",
    "    - Hierarchical clustering is performed using the 'ward' method.\n",
    "    - The clustering results are visualized using dendrograms and 3D scatter plots.\n",
    "    - The identified planes are saved to Excel and feather files.\n",
    "    \"\"\"\n",
    "    \n",
    "    Data=pd.read_feather(Path(settings['OutFolder'],'%s_Stats0.fth' % settings['Case']))\n",
    "    Block = Data['Block'].drop_duplicates().to_list()\n",
    "    locations=Data.loc[:,['X (mm)']]\n",
    "    \n",
    "    #BlockList=['B00']\n",
    "    #display(Data.loc[Data['Block'].isin(BlockList)])\n",
    "    #locations=Data.loc[Data['Block'].isin(BlockList),['X (mm)']]\n",
    "\n",
    "    plt.close(fig='all')\n",
    "\n",
    "    # Display dendograms\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(\"Plane dendogram\")\n",
    "    dend = shc.dendrogram(shc.linkage(locations, method='ward'),no_plot=False)\n",
    "    \n",
    "    # Run clustering\n",
    "    thresh=0.25\n",
    "    cluster = AgglomerativeClustering(n_clusters=None, compute_full_tree=True,\n",
    "                                      distance_threshold=thresh,compute_distances=True,\n",
    "                                      metric='euclidean', linkage='complete')\n",
    "\n",
    "    cluster.fit_predict(locations)\n",
    "    #display(len(locations),cluster.n_clusters_,cluster.n_clusters,cluster.labels_,cluster.children_,cluster.distances_)\n",
    "    Cluster=pd.DataFrame([cluster.children_[:,0],cluster.children_[:,1],cluster.distances_,cluster.labels_]).T\n",
    "    Cluster.columns=['Start','End','Distance','Label']\n",
    "\n",
    "    #display(len(Cluster),cluster.n_clusters_,cluster.n_clusters,cluster.labels_)\n",
    "    #Display(Cluster)\n",
    "    \n",
    "    # Categorize\n",
    "    Cl=pd.DataFrame(columns=['cl','X (mm)'])\n",
    "    for cl in range(cluster.n_clusters_):\n",
    "        index=Cluster.index[Cluster['Label']==cl]\n",
    "        X=Data.loc[locations.index[index],'X (mm)']\n",
    "        Cl.loc[len(Cl)]=[cl,np.mean(X)]\n",
    "\n",
    "    Cl = Cl.astype({\"cl\": int})\n",
    "    Cl.sort_values(by=['X (mm)'],inplace=True,ascending=False,ignore_index=True)\n",
    "\n",
    "    if verbose:\n",
    "        Display(Cl)\n",
    "    print('Found %d planes' % len(Cl))\n",
    "   \n",
    "    # Show plane clusters \n",
    "    fig = plt.figure(figsize=(15,15),facecolor='white')\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    clrs = iter(cm.rainbow(np.linspace(0, 1, num=len(Block))))\n",
    "    for i,block in enumerate(Block):\n",
    "        #if block.name not in BlockList:\n",
    "        #    continue\n",
    "        c=next(clrs)\n",
    "        c='k'\n",
    "\n",
    "        xs=Data.loc[Data['Block']==block,'X (mm)']\n",
    "        ys=Data.loc[Data['Block']==block,'Y (mm)']\n",
    "        zs=Data.loc[Data['Block']==block,'Z (mm)']\n",
    "\n",
    "        ax.scatter(xs, ys, zs, marker='o',color=c,s=5,\n",
    "                   fc=c,ec=c,\n",
    "                   #label=block.name,\n",
    "                   depthshade=True,alpha=1,zorder=-i)\n",
    "\n",
    "    clrs = iter(cm.rainbow(np.linspace(0, 1.0, num=cluster.n_clusters_+2)))\n",
    "    for p in range(len(Cl)):\n",
    "        c=next(clrs)\n",
    "        index=Cluster.index[Cluster['Label']==Cl.loc[p,'cl']]\n",
    "        \n",
    "        ncl=len(index)\n",
    "        if ncl>=2:\n",
    "            X=Data.loc[locations.index[index],'X (mm)']\n",
    "            Y=Data.loc[locations.index[index],'Y (mm)']\n",
    "            Z=Data.loc[locations.index[index],'Z (mm)']\n",
    "            Data.loc[locations.index[index],'Plane']=p\n",
    "\n",
    "            ax.scatter(X,Y,Z,s=50,color=c,fc='none',label=('Plane %02d' % p),depthshade=None)\n",
    "\n",
    "    ax.set_xlabel('X (mm)')\n",
    "    ax.set_ylabel('Y (mm)')\n",
    "    ax.set_zlabel('Z (mm)')\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    if verbose:\n",
    "        plt.show()\n",
    "\n",
    "    Data['Plane']=Data['Plane'].astype(int)\n",
    "    Data.sort_values(by=['Plane','Z (mm)','Y (mm)'],inplace=True,ascending=(True,False,False),ignore_index=True)\n",
    "    if verbose:\n",
    "        display(Data)\n",
    "        \n",
    "    SaveXLS(settings['OutFolder'],'%s_Stats1' % settings['Case'],Data)\n",
    "    SaveFTH(settings['OutFolder'],'%s_Stats1' % settings['Case'],Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for repeated points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindRepeats(verbose=False):\n",
    "    \"\"\"\n",
    "    Identifies and visualizes repeated measurement points in a dataset using hierarchical clustering.\n",
    "    Parameters:\n",
    "    verbose (bool): If True, displays the dendrogram and 3D scatter plot of clustered points.\n",
    "    Returns:\n",
    "    None\n",
    "    The function performs the following steps:\n",
    "    1. Reads measurement data from a feather file.\n",
    "    2. Extracts unique blocks and their corresponding locations.\n",
    "    3. Displays a dendrogram of the measurement points.\n",
    "    4. Performs agglomerative hierarchical clustering on the measurement points.\n",
    "    5. Creates a DataFrame of the clustering results.\n",
    "    6. Visualizes the clustered points in a 3D scatter plot.\n",
    "    7. Identifies and marks repeated points in the dataset.\n",
    "    8. Saves the updated dataset to an Excel and feather file.\n",
    "    Notes:\n",
    "    - The function assumes the existence of certain global variables and functions such as `OutFolder`, `settings`, `SaveXLS`, and `SaveFTH`.\n",
    "    - The clustering threshold is set to 0.25.\n",
    "    - The function uses the 'ward' method for linkage in the dendrogram and 'complete' linkage for clustering.\n",
    "    \"\"\"\n",
    "    \n",
    "    Data=pd.read_feather(Path(settings['OutFolder'],'%s_Stats1.fth' % settings['Case']))\n",
    "    Block = Data['Block'].drop_duplicates().to_list()\n",
    "    locations=Data.loc[:,['X (mm)','Y (mm)','Z (mm)']]\n",
    "\n",
    "    plt.close(fig='all')\n",
    "\n",
    "    # Display dendograms\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(\"Repeated points dendogram\")\n",
    "    dend = shc.dendrogram(shc.linkage(locations, method='ward'),no_plot=False)\n",
    "\n",
    "    # Run clustering\n",
    "    thresh=0.25\n",
    "    cluster = AgglomerativeClustering(n_clusters=None, compute_full_tree=True,\n",
    "                                      distance_threshold=thresh,compute_distances=True,\n",
    "                                      metric='euclidean', linkage='complete')\n",
    "\n",
    "    cluster.fit_predict(locations)\n",
    "    #display(len(locations),cluster.n_clusters_,cluster.n_clusters,cluster.labels_,cluster.children_,cluster.distances_)\n",
    "    Cluster=pd.DataFrame([cluster.children_[:,0],cluster.children_[:,1],cluster.distances_,cluster.labels_]).T\n",
    "    Cluster.columns=['Start','End','Distance','Label']\n",
    "\n",
    "    #display(len(Cluster),cluster.n_clusters_,cluster.n_clusters,cluster.labels_)\n",
    "    #Display(Cluster)\n",
    "\n",
    "    # Show repeated points\n",
    "    fig = plt.figure(figsize=(15,15),facecolor='white')\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    clrs = iter(cm.rainbow(np.linspace(0, 1, num=len(Block))))\n",
    "    for i,block in enumerate(Block):\n",
    "        c=next(clrs)\n",
    "\n",
    "        xs=Data.loc[Data['Block']==block,'X (mm)']\n",
    "        ys=Data.loc[Data['Block']==block,'Y (mm)']\n",
    "        zs=Data.loc[Data['Block']==block,'Z (mm)']\n",
    "\n",
    "        ax.scatter(xs, ys, zs, marker='o',color=c,s=20,\n",
    "                   fc=c,ec='k',label=block,\n",
    "                   depthshade=True,alpha=1,zorder=-i)\n",
    "\n",
    "    for cl in range(cluster.n_clusters_):\n",
    "        index=Cluster.index[Cluster['Label']==cl]\n",
    "        ovl=''\n",
    "        for ind in index:\n",
    "            ovl+=Data.loc[ind,'File']+'/'\n",
    "        ovl=ovl[:-1]\n",
    "        ncl=len(index)\n",
    "        if ncl>=2:\n",
    "            X=Data.loc[locations.index[index],'X (mm)']\n",
    "            Y=Data.loc[locations.index[index],'Y (mm)']\n",
    "            Z=Data.loc[locations.index[index],'Z (mm)']\n",
    "            c=Cluster.loc[index,'Label'].values.astype(int)\n",
    "            ax.scatter(X,Y,Z,s=30*ncl,c='red',fc='none',depthshade=None)\n",
    "        for ind in index:\n",
    "            Data.loc[index,'Repeat']=ovl\n",
    "\n",
    "    ax.set_xlabel('X (mm)')\n",
    "    ax.set_ylabel('Y (mm)')\n",
    "    ax.set_zlabel('Z (mm)')\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    if verbose:\n",
    "        plt.show()\n",
    "\n",
    "    SaveXLS(settings['OutFolder'],'%s_Stats2' % settings['Case'],Data)\n",
    "    SaveFTH(settings['OutFolder'],'%s_Stats2' % settings['Case'],Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for corresponding vertical/horizontal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMatches(verbose=False):\n",
    "    \"\"\"\n",
    "    Find and visualize matching points in a dataset based on specific conditions.\n",
    "    Parameters:\n",
    "    verbose (bool): If True, displays the plots and data. Default is False.\n",
    "    Returns:\n",
    "    None\n",
    "    This function performs the following steps:\n",
    "    1. Reads data from a feather file.\n",
    "    2. Identifies points based on specific conditions and assigns orientations.\n",
    "    3. Adjusts the coordinates of certain points.\n",
    "    4. Visualizes the points in a 3D scatter plot.\n",
    "    5. Displays dendrograms for hierarchical clustering.\n",
    "    6. Performs agglomerative clustering on the points.\n",
    "    7. Visualizes the clustered points in a 3D scatter plot.\n",
    "    8. Updates the data with radial matching information.\n",
    "    9. Sorts the data and saves it to Excel and feather files.\n",
    "    Note:\n",
    "    - The function assumes the existence of certain global variables and functions such as `pd`, `Path`, `OutFolder`, `settings`, `plt`, `shc`, `AgglomerativeClustering`, `cm`, `SaveXLS`, and `SaveFTH`.\n",
    "    - The function modifies the input data in place and saves the results to specified output files.\n",
    "    \"\"\"\n",
    "        \n",
    "    Data=pd.read_feather(Path(settings['OutFolder'],'%s_Stats2.fth' % settings['Case']))\n",
    "    locations=Data.loc[:,['X (mm)','Y (mm)','Z (mm)']]\n",
    "\n",
    "    plt.close(fig='all')\n",
    "    \n",
    "    condVu=(np.abs(Data['Y (mm)'])<0.05) & (Data['Z (mm)']>=0)\n",
    "    condVd=(np.abs(Data['Y (mm)'])<0.05) & (Data['Z (mm)']<0)\n",
    "    condHl=(np.abs(Data['Z (mm)'])<0.05) & (Data['Y (mm)']<=0)\n",
    "    condHr=(np.abs(Data['Z (mm)'])<0.05) & (Data['Y (mm)']>0)\n",
    "\n",
    "    Data.loc[condVu,'Orientation']='Vu'\n",
    "    Data.loc[condVd,'Orientation']='Vd'\n",
    "    Data.loc[condHl,'Orientation']='Hl'\n",
    "    Data.loc[condHr,'Orientation']='Hr'\n",
    "\n",
    "    DataVu=Data.loc[condVu]\n",
    "    DataVd=Data.loc[condVd]\n",
    "    DataHl=Data.loc[condHl]\n",
    "    DataHr=Data.loc[condHr]\n",
    "\n",
    "    #Display(locations[:40])\n",
    "    for cond in [condHr, condHl]:\n",
    "        locations.loc[cond,'temp']=locations.loc[cond,'Y (mm)']\n",
    "        locations.loc[cond,'Y (mm)']=locations.loc[cond,'Z (mm)']\n",
    "        locations.loc[cond,'Z (mm)']=locations.loc[cond,'temp']\n",
    "        locations=locations.drop('temp',axis=1)\n",
    "\n",
    "    locations.loc[condHl,'Z (mm)']=locations.loc[condHl,'Z (mm)'].apply(lambda x: -x)\n",
    "    locations.loc[condVd,'Z (mm)']=locations.loc[condVd,'Z (mm)'].apply(lambda x: -x)\n",
    "    #Display(locations[:40])\n",
    "    \n",
    "    if verbose:\n",
    "        fig=plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111,projection='3d')\n",
    "        x=locations.loc[condVu,'X (mm)']\n",
    "        y=locations.loc[condVu,'Y (mm)']\n",
    "        z=locations.loc[condVu,'Z (mm)']\n",
    "        ax.scatter(x,y,z,s=10,marker='o',fc='none',ec='red',depthshade=None)\n",
    "        x=locations.loc[condVd,'X (mm)']\n",
    "        y=locations.loc[condVd,'Y (mm)']\n",
    "        z=locations.loc[condVd,'Z (mm)']\n",
    "        ax.scatter(x,y,z,s=60,marker='o',fc='none',ec='green',depthshade=None)\n",
    "        x=locations.loc[condHl,'X (mm)']\n",
    "        y=locations.loc[condHl,'Y (mm)']\n",
    "        z=locations.loc[condHl,'Z (mm)']\n",
    "        ax.scatter(x,y,z,s=120,marker='o',fc='none',ec='blue',depthshade=None)\n",
    "        x=locations.loc[condHr,'X (mm)']\n",
    "        y=locations.loc[condHr,'Y (mm)']\n",
    "        z=locations.loc[condHr,'Z (mm)']\n",
    "        #ax.scatter(x,y,z,s=180,fc='none',color='black',depthshade=None)\n",
    "        ax.set_xlabel('X (mm)')\n",
    "        ax.set_ylabel('Y (mm)')\n",
    "        ax.set_zlabel('Z (mm)')\n",
    "        #ax.set_zlim(0,10)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Display dendograms\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(\"Matching point dendogram\")\n",
    "    dend = shc.dendrogram(shc.linkage(locations, method='ward'), no_plot=False)\n",
    "\n",
    "    # Run clustering\n",
    "    thresh=0.25\n",
    "    cluster = AgglomerativeClustering(n_clusters=None, compute_full_tree=True,\n",
    "                                      distance_threshold=thresh,compute_distances=True,\n",
    "                                      metric='euclidean', linkage='complete')\n",
    "\n",
    "    cluster.fit_predict(locations)\n",
    "    #display(len(locations),cluster.n_clusters_,cluster.n_clusters,cluster.labels_,cluster.children_,cluster.distances_)\n",
    "    Cluster=pd.DataFrame([cluster.children_[:,0],cluster.children_[:,1],cluster.distances_,cluster.labels_]).T\n",
    "    Cluster.columns=['Start','End','Distance','Label']\n",
    "\n",
    "    #display(len(Cluster),cluster.n_clusters_,cluster.n_clusters,cluster.labels_)\n",
    "    #Display(Cluster)\n",
    "\n",
    "    # Show matching points\n",
    "    fig = plt.figure(figsize=(15,15),facecolor='white')\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    xs=locations['X (mm)']\n",
    "    ys=locations['Y (mm)']\n",
    "    zs=locations['Z (mm)']\n",
    "\n",
    "    ax.scatter(xs, ys, zs, marker='o',color='k',s=5,\n",
    "               fc='k',ec='k',\n",
    "               depthshade=True,alpha=1,zorder=0)\n",
    "\n",
    "    colors = iter(cm.rainbow(np.linspace(0, 1, num=cluster.n_clusters_)))\n",
    "    for cl in range(cluster.n_clusters_):\n",
    "        index=Cluster.index[Cluster['Label']==cl]\n",
    "        ovl=''\n",
    "        for ind in index:\n",
    "            ovl+=Data.loc[ind,'File']+'/'\n",
    "        ovl=ovl[:-1]\n",
    "        ncl=len(index)\n",
    "        if ncl>=2:\n",
    "            color=next(colors)\n",
    "            X=locations.loc[locations.index[index],'X (mm)']\n",
    "            Y=locations.loc[locations.index[index],'Y (mm)']\n",
    "            Z=locations.loc[locations.index[index],'Z (mm)']\n",
    "            c=Cluster.loc[index,'Label'].values.astype(int)\n",
    "            ax.scatter(X,Y,Z,s=30,color='red',fc='none',\n",
    "                       depthshade=None)\n",
    "        for ind in index:\n",
    "            Data.loc[index,'Radial matching']=ovl\n",
    "\n",
    "    ax.set_xlabel('X (mm)')\n",
    "    ax.set_ylabel('Y (mm)')\n",
    "    ax.set_zlabel('Z (mm)')\n",
    "    \n",
    "    if verbose:\n",
    "        plt.show()\n",
    "\n",
    "    Data.sort_values(by=['Plane','Point'],inplace=True,ascending=True,ignore_index=True)\n",
    "    if verbose:\n",
    "        display(Data)\n",
    "        \n",
    "    SaveXLS(settings['OutFolder'],'%s_Stats3' % settings['Case'],Data)\n",
    "    SaveFTH(settings['OutFolder'],'%s_Stats3' % settings['Case'],Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Collect velocity components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ReadCsv(block,src,Ch,file):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and processes its data based on the provided parameters.\n",
    "    Parameters:\n",
    "    block (str): The block name or identifier used to construct the file path.\n",
    "    src (int): The source channel number used to filter and process the data.\n",
    "    Ch (list of int or None): A list of channel numbers to be processed. If None, no additional channels are processed.\n",
    "    file (str): The name of the CSV file to be read.\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the processed data from the CSV file.\n",
    "    Notes:\n",
    "    - The function constructs the file path using the provided block and file parameters.\n",
    "    - It reads the CSV file into a DataFrame, skipping the first row and using the second row as the header.\n",
    "    - If Ch is not None, it processes each channel in Ch by aligning its time data with the source channel's time data.\n",
    "    - The function drops rows where all elements are NaN and returns the cleaned DataFrame.\n",
    "    \"\"\"\n",
    "        \n",
    "    file=Path(settings['DataFolder'],block,'CH%d' % src,file)\n",
    "    #display(file)\n",
    "    data=pd.read_csv(file,sep=',',skipinitialspace=True,skiprows=[0],\n",
    "                     header=0,nrows=None,low_memory=False)\n",
    "    if Ch!=None:\n",
    "        for ch in Ch:\n",
    "            #Display(data[:50])\n",
    "            #display(data[5340:])\n",
    "            #print(file)\n",
    "            #print((~np.isnan(data['Time Ch. 1 (sec)'])).sum())\n",
    "            ach=(data[['Analog Extern Ch. %d' % ch,'Analog Extern Time %d (sec)' % ch]])\n",
    "            dif=ach[ach['Analog Extern Time %d (sec)' % ch].isin(data['Time Ch. %d (sec)' % src])]\n",
    "            dif=dif.drop_duplicates(subset='Analog Extern Time %d (sec)' % ch,keep='first')\n",
    "            dif.sort_values(by=['Analog Extern Time %d (sec)' % ch],inplace=True)\n",
    "        \n",
    "            #dif.dropna(inplace=True)\n",
    "            dif=dif.reset_index(drop=False)\n",
    "            #display(dif)\n",
    "            data['Analog Extern Time %d (sec)' % ch]=dif['Analog Extern Time %d (sec)' % ch]\n",
    "            data['Analog Extern Ch. %d' % ch]=dif['Analog Extern Ch. %d' % ch]\n",
    "    \n",
    "    data.dropna(how='all',inplace=True)\n",
    "    #display(data.info())\n",
    "\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LoadPointData(point, verbose=False):\n",
    "    \"\"\"\n",
    "    Loads and processes point data from a CSV file.\n",
    "    Args:\n",
    "        point (dict): A dictionary containing 'File' and 'Block' keys with corresponding values.\n",
    "        verbose (bool, optional): If True, displays the initial data for both channels. Defaults to False.\n",
    "    Returns:\n",
    "        tuple: A tuple containing two pandas DataFrames (ch1, ch2) with processed data.\n",
    "    The function performs the following steps:\n",
    "    1. Constructs the file name from the 'File' value in the point dictionary.\n",
    "    2. Reads data from the CSV file for two channels using the ReadCsv function.\n",
    "    3. Displays the initial data for both channels if verbose is True.\n",
    "    4. Converts columns with object data types to numeric, coercing errors to NaN.\n",
    "    5. Drops rows where all elements are NaN.\n",
    "    6. Checks for non-numeric values in the original data and prints a message if any are found.\n",
    "    \"\"\"\n",
    "    \n",
    "    file=point['File'].values[0]+'.csv'\n",
    "    block=point['Block'].values[0]\n",
    "    \n",
    "    ch1=ReadCsv(block,1,settings['ExternalChannels'],file)\n",
    "    ch2=ReadCsv(block,2,settings['ExternalChannels'],file)\n",
    "    if verbose:\n",
    "        Display(ch1)\n",
    "        Display(ch2)\n",
    "    count01=len(ch1)\n",
    "    count02=len(ch2)\n",
    "    #display(ch1.info())\n",
    "    cols = ch1.columns[ch1.dtypes.eq('object')]\n",
    "    #print(cols)\n",
    "    ch1[cols] = ch1[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    ch1.dropna(how='all',inplace=True)\n",
    "    ch1 = ch1.loc[ch1['Velocity Ch. 1 (m/sec)'].notnull()]\n",
    "    #display(ch1.info())\n",
    "    cols = ch2.columns[ch2.dtypes.eq('object')]\n",
    "    ch2[cols] = ch2[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    ch2.dropna(how='all',inplace=True)\n",
    "    ch2 = ch2.loc[ch2['Velocity Ch. 2 (m/sec)'].notnull()]\n",
    "    #ch2.dropna(inplace=True)\n",
    "    #display(ch1.info(),ch2.info())\n",
    "    count11=len(ch1)\n",
    "    count12=len(ch2)\n",
    "\n",
    "    if count01!=count11:\n",
    "        print(\">>> Invalid value in %s CH1: in %d, out %d (%d)\" % (file,count01,count11,count01-count11))\n",
    "    else:\n",
    "        print(\"CH1 %s: in %d, out %d\" % (file,count01,count11))\n",
    "    if count02!=count12:\n",
    "        print(\">>> Invalid value in %s CH2: in %d, out %d (%d)\" % (file,count02,count12,count02-count12))\n",
    "    else:\n",
    "        print(\"CH2 %s: in %d, out %d\" % (file,count02,count12))\n",
    "    return ([ch1,ch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChannelToComponent(currow, ch, v, verbose):\n",
    "    \"\"\"\n",
    "    Adjusts the orientation and velocity of measurement channels based on the given row's orientation.\n",
    "    Parameters:\n",
    "    currow (DataFrame): A DataFrame row containing metadata about the file, plane, and orientation.\n",
    "    ch (list of DataFrame): A list containing two DataFrames, each representing a measurement channel.\n",
    "    v (list of DataFrame): A list containing two DataFrames to store the adjusted measurement data.\n",
    "    Returns:\n",
    "    tuple: A tuple containing two DataFrames with the adjusted measurement data.\n",
    "    The function performs the following operations:\n",
    "    1. Prints the file, plane, and orientation information from the current row.\n",
    "    2. Adjusts the 'RMR1 (degree)' and 'RMR2 (degree)' columns in the measurement channels based on the orientation.\n",
    "    3. Inverts the 'Velocity Ch. 2 (m/sec)' values in the second measurement channel.\n",
    "    4. Concatenates the adjusted measurement data into the provided DataFrames `v`.\n",
    "    Note:\n",
    "    - The 'Orientation' column in `currow` can have values 'Hl', 'Vd', or 'Hr', which correspond to offsets of 90, 180, and 270 degrees respectively.\n",
    "    - If the DataFrames in `v` are empty, they are initialized with the adjusted measurement data.\n",
    "    \"\"\"\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"File:\",currow['File'].values[0],\n",
    "              \"\\nPlane:\",currow['Plane'].values[0],\n",
    "              \"\\nOrientation:\",currow['Orientation'].values[0])\n",
    "\n",
    "    offset = 0.0\n",
    "    if currow['Orientation'].values[0] == 'Hl':\n",
    "        offset = 90.0\n",
    "    if currow['Orientation'].values[0] == 'Vd':\n",
    "        offset = 180.0\n",
    "    if currow['Orientation'].values[0] == 'Hr':\n",
    "        offset = 270.0\n",
    "    ch[0]['RMR1 (degree)']=ch[0]['RMR1 (degree)'].apply(lambda x: x+offset)\n",
    "    ch[1]['RMR2 (degree)']=ch[1]['RMR2 (degree)'].apply(lambda x: x+offset)\n",
    "\n",
    "    ch[1]['Velocity Ch. 2 (m/sec)']=ch[1]['Velocity Ch. 2 (m/sec)'].apply(lambda x: -x)\n",
    "\n",
    "    if len(ch[0]):\n",
    "        if len(v[0])==0:\n",
    "            v[0]=ch[0][v[0].columns].copy()\n",
    "        else:\n",
    "            v[0]=pd.concat([v[0],ch[0]], axis=0, join='inner')\n",
    "    if len(ch[1]):\n",
    "        if len(v[1])==0:\n",
    "            v[1]=ch[1][v[1].columns].copy()\n",
    "        else:\n",
    "            v[1]=pd.concat([v[1],ch[1]], axis=0, join='inner')\n",
    "\n",
    "    return (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPointVelocity(currow, v, verbose):\n",
    "    \"\"\"\n",
    "    Calculate the point velocity from the given data.\n",
    "    Args:\n",
    "        currow (DataFrame): The current row of data containing 'File' and 'Orientation' information.\n",
    "        v (list): A list of velocity components.\n",
    "        verbose (bool): If True, display additional information for debugging purposes.\n",
    "    Returns:\n",
    "        list: A list containing the processed velocity components for channels 1 and 2.\n",
    "    Notes:\n",
    "        - The function loads point data using the `LoadPointData` function.\n",
    "        - If `verbose` is True, it displays the head of the data for both channels.\n",
    "        - The function converts the channel data to velocity components using `ChannelToComponent`.\n",
    "        - It processes the velocity data by removing outliers based on the mean and standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"Currow\",currow['File'],currow['Orientation'])\n",
    "    Ch = LoadPointData(currow,verbose)\n",
    "    #print(\"ch1:\",len(Ch[0]))\n",
    "    #print(\"ch2:\",len(Ch[1]))\n",
    "    if verbose:\n",
    "        Display(Ch[0].head())\n",
    "        Display(Ch[1].head())\n",
    "    #SaveXLS(settings['OutFolder'],'ch1',Ch[0])\n",
    "    #SaveXLS(settings['OutFolder'],'ch2',Ch[1])\n",
    "    V = ChannelToComponent(currow,Ch,v,verbose)\n",
    "\n",
    "    Lbl = ['Velocity Ch. 1 (m/sec)', 'Velocity Ch. 2 (m/sec)']\n",
    "    for i, lbl in enumerate(Lbl):\n",
    "        \n",
    "        v = V[i].dropna(how='all')\n",
    "        if len(v)<2:\n",
    "            if not len(v):\n",
    "                print('No data for %s in %s' % (lbl,currow['File']))\n",
    "            continue\n",
    "        mean=np.mean(v[lbl])\n",
    "        std_deviation = np.std(v[lbl], axis=0)\n",
    "        V[i] = v.drop(v[v[lbl]-mean>settings['nStd']*std_deviation].index)\n",
    "        if verbose:\n",
    "            print('Mean:',mean)\n",
    "            print('Std deviation:',std_deviation)\n",
    "            print('Data:',len(v),'Cleaned:',len(V[i]))\n",
    "    return (V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DataStats(data, file, labels, v):\n",
    "    \"\"\"\n",
    "    Computes and updates statistical metrics for a given dataset.\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The DataFrame containing the data to be updated.\n",
    "    file (str): The file identifier to locate the specific row in the DataFrame.\n",
    "    labels (list of str): A list of column names where the computed statistics will be stored.\n",
    "                          The list should contain at least 5 elements corresponding to:\n",
    "                          [count, mean, standard deviation, skewness, kurtosis].\n",
    "    v (array-like): The data values for which the statistics are to be computed.\n",
    "    Returns:\n",
    "    pandas.DataFrame: The updated DataFrame with the computed statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    mean = np.mean(v)\n",
    "    variance = np.var(v)\n",
    "    std_deviation = np.std(v, axis=0)\n",
    "    skewness = np.mean(((v - mean) / std_deviation) ** 3)\n",
    "    flatness = np.mean(((v - mean) / std_deviation) ** 4)\n",
    "\n",
    "    data.loc[data['File']==file,labels[0]] = len(v)\n",
    "    data.loc[data['File']==file,labels[1]] = mean\n",
    "    data.loc[data['File']==file,labels[2]] = std_deviation\n",
    "    data.loc[data['File']==file,labels[3]] = skewness\n",
    "    data.loc[data['File']==file,labels[4]] = flatness\n",
    "\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ExtractVelocityField(Data, verbose=False):\n",
    "    \"\"\"\n",
    "    Extracts the velocity field from the given data and updates the Data DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    Data (pd.DataFrame): The main DataFrame containing all the data.\n",
    "    data (pd.DataFrame): The DataFrame containing the specific data to be processed.\n",
    "    concat (bool, optional): If True, concatenates the velocity data. Defaults to True.\n",
    "    verbose (bool, optional): If True, prints detailed information during processing. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated Data DataFrame with the processed velocity field data.\n",
    "    \"\"\"\n",
    "\n",
    "    #Display(data)\n",
    "    #display(data.info())\n",
    "    Lbls_Stats=[]\n",
    "    Lbls_Time=[]\n",
    "    for i in range(2):\n",
    "        lbls = [f'Samples Ch. {i+1}',f'V_Mean Ch. {i+1} (m/sec)',f'V_RMS Ch. {i+1} (m/sec)',\n",
    "               f'V_Skewness Ch. {i+1} ()',f'V_Flatness Ch. {i+1} ()']\n",
    "        Lbls_Stats.append(lbls)\n",
    "\n",
    "        lbls = [f'Time Ch. {i+1} (sec)', f'RMR{i+1} (degree)',f'Velocity Ch. {i+1} (m/sec)']\n",
    "        Lbls_Time.append(lbls)\n",
    "\n",
    "    with tqdm(total=len(Data),dynamic_ncols=True,desc=Data.loc[Data.index[0],'File']) as pbar:\n",
    "\n",
    "        for _,Row in Data.iterrows():\n",
    "            Repeat=Row['Repeat'].split('/')\n",
    "            Matching=Row['Radial matching'].split('/')\n",
    "            if verbose:\n",
    "                print(\"Repeat:\",Repeat)\n",
    "                print(\"Matching:\",Matching)\n",
    "\n",
    "            pbar.desc=Row['File']\n",
    "            pbar.update()\n",
    "\n",
    "            v1 = pd.DataFrame(columns=Lbls_Time[0])\n",
    "            v2 = pd.DataFrame(columns=Lbls_Time[1])\n",
    "            V = [v1,v2]\n",
    "\n",
    "            for repeat in Repeat:\n",
    "                row=Data.loc[Data['File']==repeat]\n",
    "                if row['Processed'].values==1:\n",
    "                    if verbose:\n",
    "                        display('Skipped repeat %s' % repeat)\n",
    "                    continue\n",
    "\n",
    "                if verbose:\n",
    "                    display('Repeat: '+repeat)\n",
    "\n",
    "                V = GetPointVelocity(row,V,verbose)\n",
    "                Data.loc[Data['File']==repeat,'Processed']=1\n",
    "                if verbose:\n",
    "                    print(V[0].info())\n",
    "                    print(V[1].info())\n",
    "\n",
    "                for i in range(2):\n",
    "                    if len(V[i]):\n",
    "                        Data=DataStats(Data,repeat,Lbls_Stats[i],V[i]['Velocity Ch. %d (m/sec)' % (i+1)])\n",
    "            Data.loc[Data['File']==Repeat[0],'Processed']=2\n",
    "            \n",
    "            if verbose:\n",
    "                print(V[0].info())\n",
    "                print(V[1].info())\n",
    "                print('Done')\n",
    "            for i in range(2):\n",
    "                V[i]=V[i].reset_index(drop=True)\n",
    "\n",
    "            outFolder=Path(settings['OutFolder'],'VField')\n",
    "            outFolder.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "            V = pd.concat([V[0],V[1]], axis=1).reset_index(drop=True)\n",
    "            V = V.dropna(how='all')\n",
    "           \n",
    "            if len(V):\n",
    "                SaveFTH(outFolder,Row['File'],V,verbose=False)\n",
    "                SaveCSV(outFolder,Row['File'],V,verbose=False)\n",
    "                #SaveMAT(outFolder,Row['File'],v2,verbose=False)\n",
    "\n",
    "    return (Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(data):\n",
    "    \"\"\"\n",
    "    Calculate the difference between sample data and valid velocity count, \n",
    "    and the difference between various velocity statistics.\n",
    "    Parameters:\n",
    "    data (DataFrame): A pandas DataFrame containing the following columns:\n",
    "        - 'Samples Ch. 1', 'Samples Ch. 2': Sample data for channels 1 and 2.\n",
    "        - 'Valid Vel.Count Ch. 1', 'Valid Vel.Count Ch. 2': Valid velocity count for channels 1 and 2.\n",
    "        - 'V_Mean Ch. 1 (m/sec)', 'V_Mean Ch. 2 (m/sec)': Mean velocity for channels 1 and 2.\n",
    "        - 'V_RMS Ch. 1 (m/sec)', 'V_RMS Ch. 2 (m/sec)': RMS velocity for channels 1 and 2.\n",
    "        - 'V_Skewness Ch. 1 ()', 'V_Skewness Ch. 2 ()': Skewness of velocity for channels 1 and 2.\n",
    "        - 'V_Flatness Ch. 1 ()', 'V_Flatness Ch. 2 ()': Flatness of velocity for channels 1 and 2.\n",
    "        - 'Velocity Mean Ch. 1 (m/sec)', 'Velocity Mean Ch. 2 (m/sec)': Mean velocity for channels 1 and 2.\n",
    "        - 'Velocity RMS Ch. 1 (m/sec)', 'Velocity RMS Ch. 2 (m/sec)': RMS velocity for channels 1 and 2.\n",
    "        - 'Velocity Skewness Ch. 1 ()', 'Velocity Skewness Ch. 2 ()': Skewness of velocity for channels 1 and 2.\n",
    "        - 'Velocity Flatness Ch. 1 ()', 'Velocity Flatness Ch. 2 ()': Flatness of velocity for channels 1 and 2.\n",
    "    Returns:\n",
    "    DataFrame: The input DataFrame with additional columns:\n",
    "        - 'diff Samples Ch. 1', 'diff Samples Ch. 2': Difference between sample data and valid velocity count for channels 1 and 2.\n",
    "        - 'Diff Velocity Mean Ch. 1 (m/sec)', 'Diff Velocity Mean Ch. 2 (m/sec)': Difference in mean velocity for channels 1 and 2.\n",
    "        - 'Diff Velocity RMS Ch. 1 (m/sec)', 'Diff Velocity RMS Ch. 2 (m/sec)': Difference in RMS velocity for channels 1 and 2.\n",
    "        - 'Diff Velocity Skewness Ch. 1 ()', 'Diff Velocity Skewness Ch. 2 ()': Difference in skewness of velocity for channels 1 and 2.\n",
    "        - 'Diff Velocity Flatness Ch. 1 ()', 'Diff Velocity Flatness Ch. 2 ()': Difference in flatness of velocity for channels 1 and 2.\n",
    "    \"\"\"\n",
    "    stats = [['Mean','m/sec'], ['RMS','m/sec'], ['Skewness',''], ['Flatness','']]\n",
    "   \n",
    "    for i in range(2):\n",
    "        data[f'diff Samples Ch. {i+1}']=data[f'Samples Ch. {i+1}']-data[f'Valid Vel.Count Ch. {i+1}']\n",
    "        for stat, unit in stats:\n",
    "            data[f'Diff Velocity {stat} Ch. {i+1} ({unit})'] = data[f'V_{stat} Ch. {i+1} ({unit})'] - data[f'Velocity {stat} Ch. {i+1} ({unit})']\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeStats(verbose=False):\n",
    "    \"\"\"\n",
    "    Compute and save statistical analysis of velocity field data.\n",
    "    This function reads velocity field data from a Feather file, processes it to extract\n",
    "    the velocity field, computes differences, and saves the results in both Feather and\n",
    "    Excel formats. Optionally, it prints the maximum absolute values of axial, radial, \n",
    "    and tangential velocities for different orientations.\n",
    "    Args:\n",
    "        verbose (bool): If True, prints the maximum absolute values of velocities for \n",
    "                        different orientations. Default is False.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    Data=pd.read_feather(Path(settings['OutFolder'],'%s_Stats3.fth' % settings['Case']))\n",
    "    Data['Processed']=0\n",
    "    \n",
    "    Data = ExtractVelocityField(Data,verbose=verbose)\n",
    "    Data = Data.loc[Data['Processed']==1]\n",
    "    Data = Data.reset_index(drop=True)\n",
    "    \n",
    "    Data = Diff(Data)\n",
    "    \n",
    "    ##display(Data)\n",
    "    \n",
    "    SaveFTH(Path(settings['OutFolder'],'.'),'%s_Stats4' % settings['Case'],Data,verbose=False)\n",
    "    SaveXLS(Path(settings['OutFolder'],'.'),'%s_Stats4' % settings['Case'],Data,verbose=False)\n",
    "\n",
    "    if verbose:\n",
    "        print('Axial Vu =', np.abs(Data.loc[Data['Orientation'] =='Vu','V_Mean Ch. 2 (m/sec)']).mean())\n",
    "        print('Axial Vd =', np.abs(Data.loc[Data['Orientation'] =='Vd','V_Mean Ch. 2 (m/sec)']).mean())\n",
    "        print('Axial Hl =', np.abs(Data.loc[Data['Orientation'] =='Hl','V_Mean Ch. 2 (m/sec)']).mean())\n",
    "        print('Axial Hr =', np.abs(Data.loc[Data['Orientation'] =='Hr','V_Mean Ch. 2 (m/sec)']).mean())\n",
    "        print('Radial Vu =', np.abs(Data.loc[Data['Orientation'] =='Vu','V_Mean Ch. 1 (m/sec)']).mean())\n",
    "        print('Radial Vd =', np.abs(Data.loc[Data['Orientation'] =='Vd','V_Mean Ch. 1 (m/sec)']).mean())\n",
    "        print('Tangential Hl =', np.abs(Data.loc[Data['Orientation'] =='Hl','V_Mean Ch. 1 (m/sec)']).mean())\n",
    "        print('Tangential Hr =', np.abs(Data.loc[Data['Orientation'] =='Hr','V_Mean Ch. 1 (m/sec)']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Phase analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetIntervals(period,step,wl,wr):\n",
    "    \"\"\"\n",
    "    Generates intervals and their centers based on the given period, step, and widths.\n",
    "\n",
    "    Args:\n",
    "        period (float): The total period over which intervals are generated.\n",
    "        step (float): The step size between interval centers.\n",
    "        wl (float): The left width from the interval center.\n",
    "        wr (float): The right width from the interval center.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - intervals (pd.arrays.IntervalArray): The generated intervals.\n",
    "            - centers (np.ndarray): The centers of the intervals.\n",
    "    \"\"\"\n",
    "    \n",
    "    left=np.array([])\n",
    "    right=np.array([])\n",
    "    ctrs=np.arange(0,period,step)\n",
    "\n",
    "    for ctr in ctrs:\n",
    "        left=np.append(left,ctr-wl)\n",
    "        right=np.append(right,ctr+wr)\n",
    "\n",
    "    intervals=pd.arrays.IntervalArray.from_arrays(left,right,closed=settings['IntervalClosed'])\n",
    "    #display(intervals)\n",
    "\n",
    "    return (intervals,ctrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetPhase(v, row):\n",
    "    \"\"\"\n",
    "    Adjusts the phase of the given DataFrame `v` based on the orientation specified in `row`.\n",
    "    Parameters:\n",
    "    v (pd.DataFrame): DataFrame containing phase data to be adjusted.\n",
    "    row (pd.Series): Series containing the 'Orientation' key to determine the phase offset.\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame `v` with adjusted phase values.\n",
    "    The function uses the 'Orientation' value from `row` to determine the appropriate phase offset from the `settings` dictionary.\n",
    "    It then applies this offset to the 'RMR1 (degree)' and 'RMR2 (degree)' columns of the DataFrame `v`, using modulo operation with the given `period`.\n",
    "    \"\"\"\n",
    "      \n",
    "    period = settings['Period']\n",
    "     \n",
    "    orientation = row['Orientation']\n",
    "    if orientation == 'Vu':\n",
    "        offset = settings['VerticalUpPhaseOffset']\n",
    "    if orientation == 'Vd':\n",
    "        offset = settings['VerticalDownPhaseOffset']\n",
    "    if orientation == 'Hl':\n",
    "        offset = settings['HorizontalLeftPhaseOffset']\n",
    "    if orientation == 'Hr':\n",
    "        offset = settings['HorizontalRightPhaseOffset']\n",
    "\n",
    "    for i in range(2):\n",
    "        if len(v['RMR%d (degree)' % (i+1)].dropna(how='all')) > 0:\n",
    "            v['RMR%d (degree)' % (i+1)]=v['RMR%d (degree)' % (i+1)].apply(lambda x: np.mod(x+offset,period))\n",
    "\n",
    "    return(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plot point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPointData(v, row):\n",
    "    \"\"\"\n",
    "    Plots point data from a DataFrame based on the given row's orientation and file information.\n",
    "    Parameters:\n",
    "    v (pd.DataFrame): The DataFrame containing the point data to be plotted. The first three columns are considered as one set of data (v1) and the remaining columns as another set (v2).\n",
    "    row (pd.Series): A Series containing metadata for the plot. It should have the keys 'Orientation' and 'File'.\n",
    "    The function creates a figure with three subplots:\n",
    "    1. Scatter plot of the first two columns of v1 and v2.\n",
    "    2. Scatter plot of the first and third columns of v1.\n",
    "    3. Scatter plot of the first and third columns of v2.\n",
    "    The title of the plot is derived from the 'File' and 'Orientation' values in the row.\n",
    "    \"\"\"\n",
    "    \n",
    "    v1 = v[v.columns[:3]]\n",
    "    v2 = v[v.columns[3:]]\n",
    "    #display(v1)\n",
    "    #display(v2)\n",
    "    orientation = row['Orientation']\n",
    "    title = '%s' % row['File'] + ' ' + \\\n",
    "        ('(Up)' if orientation == 'Vu' else '(Down)' if orientation == 'Vd' else '(Left)' if orientation == 'Hl' else '(Right)')\n",
    "\n",
    "    fig = plt.figure(figsize=(18,12),facecolor='white')\n",
    "    ax = fig.add_subplot(311)\n",
    "    ax.scatter(v1[v1.columns[1]],v1[v1.columns[0]],c='r',s=0.1)\n",
    "    ax.scatter(v2[v2.columns[1]],v2[v2.columns[0]],c='g',s=0.1)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel(v1.columns[0])\n",
    "    #ax.set_xlim(0,540)\n",
    "    plt.title(title, size=24)\n",
    "\n",
    "    ax = fig.add_subplot(312)\n",
    "    ax.scatter(v1[v1.columns[1]],v1[v1.columns[2]],c='r',s=0.1)\n",
    "    ax.set_xlabel(v1.columns[1])\n",
    "    ax.set_ylabel(v1.columns[2])\n",
    "    #ax.set_xlim(0,540)\n",
    "\n",
    "    ax = fig.add_subplot(313)\n",
    "    ax.scatter(v2[v2.columns[1]],v2[v2.columns[2]],c='g',s=0.1)\n",
    "    ax.set_xlabel(v2.columns[1])\n",
    "    ax.set_ylabel(v2.columns[2])\n",
    "    #ax.set_xlim(0,540)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot phase distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPhaseDistribution(v, row):\n",
    "    \"\"\"\n",
    "    Plots the phase distribution of given data.\n",
    "    Parameters:\n",
    "    V (DataFrame): A pandas DataFrame containing the phase data with columns 'RMR1 (degree)' and 'RMR2 (degree)'.\n",
    "    Row (Series): A pandas Series containing metadata for the plot, including 'Orientation' and 'File'.\n",
    "    The function generates a scatter plot of 'RMR1 (degree)' vs 'RMR2 (degree)' and titles the plot based on the 'Orientation' and 'File' values in the row.\n",
    "    \"\"\"\n",
    "   \n",
    "    orientation = row['Orientation']\n",
    "    title = ('Up' if orientation == 'Vu' else 'Down' if orientation == 'Vd' else 'Left' if orientation == 'Hl' else 'Right')\n",
    "    title = '%s (%s)' % (row['File'], title) \n",
    "        \n",
    "    if len(v) > 0:\n",
    "        fig = plt.figure(figsize=(12,10),facecolor='white')\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        theta=v['RMR1 (degree)']\n",
    "        phi=v['RMR2 (degree)']\n",
    "\n",
    "        print(len(theta),len(phi))\n",
    "        ax.scatter(theta,phi,s=0.05)\n",
    "        ax.set_xlabel('RMR1',size=12)\n",
    "        ax.set_ylabel('RMR2',size=12)\n",
    "        ax.set_title(title, size=12)\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SlotDistribution(v, intervals):\n",
    "    \"\"\"\n",
    "    Calculate the slot distribution for given periods and intervals.\n",
    "\n",
    "    Parameters:\n",
    "    period (int): The period for the slot distribution.\n",
    "    intervals (list of pd.Interval): List of intervals to be used for distribution.\n",
    "    v (pd.DataFrame): DataFrame containing the data to be analyzed. The DataFrame is expected to have at least 6 columns, \n",
    "                      where the first three columns correspond to the first set of data and the next three columns correspond \n",
    "                      to the second set of data.\n",
    "\n",
    "    Returns:\n",
    "    list of pd.DataFrame: A list containing two DataFrames, each corresponding to the slot distribution for the two sets of data.\n",
    "                          Each DataFrame includes the original data along with the calculated slot distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    period = settings['Period']\n",
    "   \n",
    "    a1 = v[v.columns[:3]]\n",
    "    a2 = v[v.columns[3:]]\n",
    "    A = [a1,a2]\n",
    "\n",
    "    V = []\n",
    "    for i,col in enumerate(['RMR1 (degree)','RMR2 (degree)']):\n",
    "        a = A[i]\n",
    "        #print('Input:',a, len(a))\n",
    "        if not len(a):\n",
    "            continue\n",
    "        Inter=pd.DataFrame(columns=intervals)\n",
    "\n",
    "        for inter in intervals[:]:\n",
    "            #display(inter)\n",
    "\n",
    "            #In=pd.DataFrame()\n",
    "            #In[inter] = a[col].apply(lambda x: x in inter)\n",
    "\n",
    "            In=pd.DataFrame()\n",
    "            In[inter] = np.where(np.logical_and(a[col]>=inter.left, a[col]<inter.right),True,False)\n",
    "            #In[inter] = a[col][cond],axis=1)\n",
    "            if inter.left<0:\n",
    "                linter = pd.Interval(inter.left+period,period,closed=settings['IntervalClosed'])\n",
    "                #display(linter)\n",
    "                In[linter] = np.where(np.logical_and(a[col]>=linter.left, a[col]<linter.right),True,False)\n",
    "            if inter.right>period:\n",
    "                rinter = pd.Interval(0,inter.right-period,closed=settings['IntervalClosed'])\n",
    "                #display(rinter)\n",
    "                In[rinter] = np.where(np.logical_and(a[col]>=rinter.left, a[col]<rinter.right),True,False)\n",
    "\n",
    "            #display(In)\n",
    "            #Inter[inter] = In.fillna(False).select_dtypes(include=['bool']).sum(axis=1)\n",
    "            Inter[inter] = In.sum(axis=1)\n",
    "        #display(a, Inter)\n",
    "        a=pd.concat([a,Inter],axis=1)\n",
    "        #df_.sort_values(by=[col],inplace=True,ascending=True,ignore_index=True)\n",
    "\n",
    "        nsamples=pd.DataFrame(columns=a.columns)\n",
    "        for inter in intervals[:]:\n",
    "            nsamples.at[0,inter]=a[inter].sum()\n",
    "        nsamples.at[0,col]=nsamples[intervals].sum(axis=1).values[0].astype(int)\n",
    "        #display(nsamples)\n",
    "        #a=pd.concat([a,nsamples],ignore_index=True)\n",
    "        if i==0:\n",
    "            V=[a.copy()]\n",
    "        else:\n",
    "            V.append(a.copy())\n",
    "\n",
    "    return(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velocity stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVStats(v, intervals):\n",
    "    \"\"\"\n",
    "    Calculate velocity statistics for given intervals.\n",
    "    Parameters:\n",
    "    v (list of pd.DataFrame): List of DataFrames containing velocity data for channels.\n",
    "    intervals (pd.IntervalIndex): IntervalIndex object defining the intervals for analysis.\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the calculated statistics for each interval.\n",
    "        Columns include:\n",
    "        - 'Slot': Interval slots.\n",
    "        - 'Angular position (deg)': Angular position in degrees.\n",
    "        - 'Ch. 1 samples': Number of samples in Channel 1.\n",
    "        - 'Ch. 1 mean': Mean velocity in Channel 1.\n",
    "        - 'Ch. 1 sdev': Standard deviation of velocity in Channel 1.\n",
    "        - 'Ch. 2 samples': Number of samples in Channel 2.\n",
    "        - 'Ch. 2 mean': Mean velocity in Channel 2.\n",
    "        - 'Ch. 2 sdev': Standard deviation of velocity in Channel 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    vstats=pd.DataFrame([],columns=['Angular position (deg)',\n",
    "                                    'Slot start (deg)','Slot end (deg)',\n",
    "                                    'Ch. 1 samples','Ch. 1 mean','Ch. 1 sdev',\n",
    "                                    'Ch. 2 samples','Ch. 2 mean','Ch. 2 sdev'])\n",
    "    vstats['Angular position (deg)']=intervals.left+settings['Wleft']\n",
    "    vstats['Slot start (deg)']=intervals.left\n",
    "    vstats['Slot end (deg)']=intervals.right\n",
    "\n",
    "    for i,col in enumerate(['Velocity Ch. 1 (m/sec)','Velocity Ch. 2 (m/sec)']):\n",
    "        var = v[i]\n",
    "        icol = i*3+3\n",
    "        #print('Input:', vstats.columns[icol], var, len(var))\n",
    "        \n",
    "        for j,inter in enumerate(intervals[:]):\n",
    "        \n",
    "            var_=var.loc[var[inter]>0][col]\n",
    "            vstats.iloc[j, icol] = var[inter].sum()\n",
    "            vstats.iloc[j, icol+1] = np.mean(var_)\n",
    "            vstats.iloc[j, icol+2] = np.std(var_)\n",
    "    \n",
    "    return (vstats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot velocity stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotVStats(ctrs,vstats,row):\n",
    "\n",
    "    orientation = row['Orientation']\n",
    "    orientation = ('Up' if orientation == 'Vu' else 'Down' if orientation == 'Vd' else 'Left' if orientation == 'Hl' else 'Right')\n",
    "    title1 = '%s (%s, Ch. 1)' % (row['File'],orientation)\n",
    "    title2 = '%s (%s, Ch. 2)' % (row['File'],orientation)\n",
    "    Src=[['Ch. 1 samples','Ch. 1 mean','Ch. 1 sdev'],\n",
    "         ['Ch. 2 samples','Ch. 2 mean','Ch. 2 sdev']]\n",
    "    Lbls=[[r'$\\theta$',r'$\\overline{V}_1\\ (m/s)$',r'$\\widetilde{V}_1\\ (m/s)$',title1],\n",
    "          [r'$\\theta$',r'$\\overline{V}_2\\ (m/s)$',r'$\\widetilde{V}_2\\ (m/s)$',title2]]\n",
    "\n",
    "    alpha=0.8\n",
    "    for src,lbl in list(zip(Src,Lbls)):\n",
    "\n",
    "        if vstats[src[0]].isnull().all():\n",
    "            continue\n",
    "\n",
    "        Vn=np.asarray(vstats[src[0]],dtype=float)\n",
    "        vm=np.asarray(vstats[src[1]],dtype=float)\n",
    "        vs=np.asarray(vstats[src[2]],dtype=float)\n",
    "        Vm=vm[~np.isnan(vm)]\n",
    "        Vs=vs[~np.isnan(vs)]\n",
    "\n",
    "        V=[Vn,Vm,Vs]\n",
    "\n",
    "        fig,axs=plt.subplots(3,1,sharex=True,tight_layout=True,figsize=(12,8),facecolor='white')\n",
    "\n",
    "        axs[-1].set_xlabel(lbl[0],size=14)\n",
    "        axs[0].set_ylabel('Counts',size=14)\n",
    "        axs[1].set_ylabel(lbl[1],size=14)\n",
    "        axs[2].set_ylabel(lbl[2],size=14)\n",
    "\n",
    "        for i,v in enumerate(V):\n",
    "            bars=[]\n",
    "            for x, h in zip(ctrs, v):\n",
    "                rect = Rectangle((x, 0), settings['Wslot'], h)\n",
    "                bars.append(rect)\n",
    "            norm = mcolors.Normalize(vmin=np.min(v), vmax=np.max(v))\n",
    "            colors = cm.jet(norm(v))\n",
    "            pc = PatchCollection(bars, facecolor=colors, alpha=alpha, edgecolor='black')\n",
    "            axs[i].add_collection(pc)\n",
    "\n",
    "            axs[i].autoscale(enable=True,axis='both',tight=False)\n",
    "\n",
    "        plt.suptitle(lbl[3],size=20)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single point Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinglePointAnalysis(row, show=True):\n",
    "    \"\"\"\n",
    "    Perform single point analysis on the given row of data.\n",
    "    This function loads velocity field data, processes it through various stages,\n",
    "    and optionally displays plots at each stage. The final statistics are saved\n",
    "    to a CSV file.\n",
    "    Parameters:\n",
    "    row (pd.Series): A pandas Series containing the data for a single point.\n",
    "    show (bool): A flag to indicate whether to display plots. Default is True.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    " \n",
    "    plt.close('all')\n",
    "    \n",
    "    v0 = LoadFTH(Path(settings['OutFolder'], 'VField'), row['File'])\n",
    "    if not len(v0):\n",
    "        return(0)\n",
    "    \n",
    "    sw = 'S%04dW%04d' % (settings['Step']*100,settings['Wslot']*100)\n",
    "    outfolder=Path(settings['OutFolder'],'PolarStats',sw,'Csv')\n",
    "    outfolder.mkdir(parents=True,exist_ok=True)\n",
    "    outfile='%s_Stats_%s_P%06d' % (settings['Case'],sw,row['Point'])\n",
    "    outfile = Path(outfolder,outfile).with_suffix('.csv')\n",
    "    #print(outfile,outfile.exists())\n",
    "    if outfile.exists() and not settings['Overwrite']:\n",
    "        return(len(v0))\n",
    "   \n",
    "    if show:\n",
    "        PlotPointData(v0,row)\n",
    "    v1 = SetPhase(v0,row)\n",
    "    if show:\n",
    "        PlotPhaseDistribution(v1,row)\n",
    "\n",
    "    Intervals,Ctrs=SetIntervals(settings['Period'],settings['Step'],settings['Wleft'],settings['Wright'])\n",
    "    v2=SlotDistribution(v1,Intervals)\n",
    "   \n",
    "    #display(((v2[0]).iloc[:,3:]).sum().sum())\n",
    "    #display(((v2[1]).iloc[:,3:]).sum().sum())\n",
    "    VStats=GetVStats(v2,Intervals)\n",
    "    #display(VStats,VStats['Ch. 1 samples'].sum(),VStats['Ch. 2 samples'].sum())\n",
    "    \n",
    "    if show:\n",
    "        PlotVStats(Ctrs,VStats,row)\n",
    "\n",
    "    VStats.to_csv(outfile,index=False)\n",
    "    \n",
    "    return(len(v0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTK exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeVtkDataset(tri, z):\n",
    "    \"\"\"\n",
    "    Creates a VTK unstructured grid dataset from a Delaunay triangulation and a specified z-coordinate.\n",
    "    Parameters:\n",
    "    tri (scipy.spatial.Delaunay): A Delaunay triangulation object containing points and simplices.\n",
    "    z (float): The z-coordinate to be assigned to all points in the dataset.\n",
    "    Returns:\n",
    "    vtkUnstructuredGrid: A VTK unstructured grid dataset with the specified points and triangles.\n",
    "    \"\"\"\n",
    "\n",
    "    vtk_dataset = vtkUnstructuredGrid()\n",
    "    pts = vtkPoints()\n",
    "    for id,pt in enumerate(tri.points):\n",
    "        x, y = pt\n",
    "        pts.InsertPoint(id, [x,y,z])\n",
    "    vtk_dataset.SetPoints(pts)\n",
    "    \n",
    "    vtk_dataset.Allocate(tri.nsimplex)\n",
    "    for point_ids in tri.simplices:\n",
    "        vtk_dataset.InsertNextCell(VTK_TRIANGLE, 3, point_ids)\n",
    "\n",
    "    return(vtk_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddArray(vtk_dataset, name, data, cnames):\n",
    "    \"\"\"\n",
    "    Adds a named array to a VTK dataset.\n",
    "    Parameters:\n",
    "    vtk_dataset (vtk.vtkDataSet): The VTK dataset to which the array will be added.\n",
    "    name (str): The name of the array to be added.\n",
    "    data (list or numpy.ndarray): The data to be added to the array. Should be a list or array of values.\n",
    "    cnames (list of str): The names of the components in the array.\n",
    "    Returns:\n",
    "    vtk.vtkDataSet: The VTK dataset with the added array.\n",
    "    \"\"\"\n",
    "\n",
    "    npoints = vtk_dataset.GetNumberOfPoints()\n",
    "    ndata = len(data)\n",
    "    \n",
    "    #ic(vtk_dataset.GetNumberOfPoints())\n",
    "    array = vtkDoubleArray()\n",
    "    array.SetName(name)\n",
    "    array.SetNumberOfComponents(ndata)\n",
    "    for i, cname in enumerate(cnames):\n",
    "        array.SetComponentName(i,cname)\n",
    "        \n",
    "    array.SetNumberOfTuples(npoints)\n",
    "    dat = np.dstack((data)).reshape(npoints,ndata)\n",
    "    #print(dat)\n",
    "    for i, val in enumerate(dat):\n",
    "        #print(val)\n",
    "        array.SetTuple(i, val)\n",
    "    vtk_dataset.GetPointData().AddArray(array)\n",
    "            \n",
    "    return(vtk_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportToVTKVtu(block, row, X):\n",
    "    \"\"\"\n",
    "    Exports data to a VTK (.vtu) file format.\n",
    "    Parameters:\n",
    "    block (tuple): A tuple containing theta, rad, Vn, Vm, Vs arrays.\n",
    "    row (dict): A dictionary containing metadata for the current row, including 'Orientation' and 'Plane'.\n",
    "    X (float): A scalar value used for normalization.\n",
    "    Returns:\n",
    "    None\n",
    "    The function performs the following steps:\n",
    "    1. Constructs the output folder path based on settings.\n",
    "    2. Extracts and normalizes the radial and axial coordinates.\n",
    "    3. Creates a Delaunay triangulation of the points.\n",
    "    4. Determines the orientation and labels for the velocity components.\n",
    "    5. Interpolates the velocity data if the orientation is 'Left' or 'Right'.\n",
    "    6. Adds the velocity data arrays to the VTK dataset.\n",
    "    7. Writes the VTK dataset to a .vtu file.\n",
    "    \"\"\"\n",
    "    \n",
    "    sw = 'S%04dW%04d' % (settings['Step']*100,settings['Wslot']*100)\n",
    "    outfolder=Path(settings['OutFolder'],'PolarStats',sw,'Vtk')\n",
    "    outfolder.mkdir(exist_ok=True)\n",
    "\n",
    "    theta,rad,Vn,Vm,Vs = block\n",
    "    \n",
    "    Z = X/settings['Rref']\n",
    "    \n",
    "    x = rad * np.cos(theta)\n",
    "    y = rad * np.sin(theta)\n",
    "    z = np.full((x.shape[0],x.shape[1]),Z)\n",
    "    points=np.vstack([x.flatten(),y.flatten()]).T\n",
    "    points=np.vstack((points,(0,0)))\n",
    "    tri = Delaunay(points)\n",
    "    \n",
    "    orientation = row['Orientation']\n",
    "    orientation = ('Up' if orientation == 'Vu' else 'Down' if orientation == 'Vd' else 'Left' if orientation == 'Hl' else 'Right')\n",
    "    if orientation in ['Up','Down']:\n",
    "        Lbl=['Radial velocity (%s)' % orientation, 'Axial velocity (%s)' % orientation]\n",
    "    if orientation in ['Left','Right']:\n",
    "        Lbl=['Tangential velocity (%s)' % orientation, 'Axial velocity (%s)' % orientation]\n",
    "        \n",
    "    points=np.vstack([x.flatten(),y.flatten()]).T\n",
    "    points=np.vstack((points,(0,0)))\n",
    "    tri = Delaunay(points)\n",
    "    \n",
    "    vtk_dataset = MakeVtkDataset(tri,Z)\n",
    "    \n",
    "    for k,lbl in enumerate(Lbl):\n",
    "\n",
    "        V=[Vn[k,:,:],Vm[k,:,:],Vs[k,:,:]]\n",
    "    \n",
    "        fact=1\n",
    "        if orientation in ['Left','Right']:\n",
    "            fact=settings['RefractiveIndexCorrection']\n",
    "            kernel=settings['Interpolation']\n",
    "            #ic(kernel)\n",
    "\n",
    "            pts=np.vstack([rad.flatten()*fact,theta.flatten()]).T\n",
    "            Rmin = pts[:,0].min()\n",
    "            Rmax = pts[:,0].max()\n",
    "            for i in range(3):\n",
    "\n",
    "                v=V[i].flatten()\n",
    "                cond = ~np.isnan(v)\n",
    "                Pts=pts[cond]\n",
    "                v=v[cond]\n",
    "                interp = RBFInterpolator(Pts,v,\n",
    "                                     smoothing=0.0,\\\n",
    "                                     kernel=kernel)\n",
    "                Pts=np.vstack([rad.flatten(),theta.flatten()]).T\n",
    "\n",
    "                V[i] = interp(Pts)\n",
    "\n",
    "                cond= (Pts[:,0]<Rmin) | (Pts[:,0]>Rmax)\n",
    "                V[i][cond] = np.nan\n",
    "\n",
    "        for i in range(3):\n",
    "            V[i] = np.append(V[i],[0])\n",
    "\n",
    "        vtk_dataset = AddArray(vtk_dataset, lbl, \n",
    "                                  [V[0],V[1],V[2]],\n",
    "                                  ['Count','Mean','RMS'])\n",
    "    \n",
    "    writer = vtkXMLUnstructuredGridWriter()\n",
    "    outfile='%s_Stats_%s_%s_P%02d' % (settings['Case'],sw,orientation,row['Plane'])\n",
    "    writer.SetFileName(Path(outfolder,outfile).with_suffix('.vtu'))\n",
    "    writer.SetInputData(vtk_dataset)\n",
    "    writer.Write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportToVTKVtm(block, data, plane, outfile):\n",
    "\n",
    "    sw = 'S%04dW%04d' % (settings['Step']*100,settings['Wslot']*100)\n",
    "    outfolder=Path(settings['OutFolder'],'PolarStats',sw,'Vtk')\n",
    "    outfolder.mkdir(exist_ok=True)\n",
    "\n",
    "    theta,rad,Vn,Vm,Vs = block\n",
    "    \n",
    "    Z = X/settings['Rref']\n",
    "   \n",
    "   \n",
    "    \n",
    "    VaR=[Vn[0,:,:],Vm[0,:,:],Vs[0,:,:]]\n",
    "    VaT=[Vn[1,:,:],Vm[1,:,:],Vs[1,:,:]]\n",
    "    Vr=[Vn[2,:,:],Vm[2,:,:],Vs[2,:,:]]\n",
    "    Vt=[Vn[3,:,:],Vm[3,:,:],Vs[3,:,:]]\n",
    "\n",
    "    #ic(VaR)\n",
    "    #V=np.sqrt(Va**2+Vr**2+Vt*+2)\n",
    "\n",
    "    vtk_block = vtkMultiBlockDataSet()\n",
    "    vtk_block.SetNumberOfBlocks(2)\n",
    "    \n",
    "    Vars = [['VaR',VaR], ['VaT',VaT], ['Vr',Vr], ['Vt',Vt]]\n",
    "    for ivar, var in enumerate(Vars):\n",
    "        var0, var1 = var\n",
    "        \n",
    "        for i in range(3):\n",
    "            var1[i] = np.append(var1[i],[0])\n",
    "            \n",
    "        fact=1\n",
    "        if var0=='VaT':\n",
    "            fact=settings['RefractiveIndexCorrection']\n",
    "            \n",
    "        x = rad * np.cos(theta)*fact\n",
    "        y = rad * np.sin(theta)*fact\n",
    "        z = np.full((x.shape[0],x.shape[1]),Z)\n",
    "        points=np.vstack([x.flatten(),y.flatten()]).T\n",
    "        points=np.vstack((points,(0,0)))\n",
    "        tri = Delaunay(points)\n",
    "        \n",
    "        vtk_dataset = MakeVtkDataset(tri,Z)\n",
    "            \n",
    "        npoints = vtk_dataset.GetNumberOfPoints()\n",
    "        \n",
    "        Stats =['Count','Mean','RMS']\n",
    "        for istat,stat in enumerate(Stats):\n",
    "           array = vtkDoubleArray()\n",
    "           array.SetName('%s %s' % (var0,stat))\n",
    "           \n",
    "           array.SetNumberOfTuples(npoints)\n",
    "           for j, val in enumerate(var1[istat]):\n",
    "               array.SetTuple(j, [val])\n",
    "           vtk_dataset.GetPointData().AddArray(array)\n",
    "    \n",
    "        vtk_block.SetBlock(ivar,vtk_dataset)\n",
    "        vtk_block.GetMetaData(ivar).Set(vtkMultiBlockDataSet.NAME(),\n",
    "                                        var0)\n",
    "                                        #'%s_P%02d' % (var0,plane))\n",
    "        \n",
    "    writer = vtkXMLMultiBlockDataWriter()\n",
    "    writer.SetFileName(Path(outfolder,outfile).with_suffix('.vtm'))\n",
    "    writer.SetInputData(vtk_block)\n",
    "    writer.Write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polar plots and VTK output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildBlocks(radii, ctrs, vstats, verbose=False):\n",
    "    \"\"\"\n",
    "    BuildBlocks constructs and returns a set of data blocks for given radii, centers, and statistics.\n",
    "    Parameters:\n",
    "    radii (numpy.ndarray): Array of radii values.\n",
    "    ctrs (numpy.ndarray): Array of center values.\n",
    "    vstats (dict): Dictionary containing statistical data with keys 'Ch. 1 samples', 'Ch. 1 mean', 'Ch. 1 sdev', \n",
    "                   'Ch. 2 samples', 'Ch. 2 mean', 'Ch. 2 sdev'.\n",
    "    verbose (bool, optional): If True, displays the vstats dictionary. Default is False.\n",
    "    Returns:\n",
    "    list: A list containing:\n",
    "        - theta (numpy.ndarray): 2D array of angular positions in radians.\n",
    "        - rad (numpy.ndarray): 2D array of radial positions.\n",
    "        - Vn (numpy.ndarray): 3D array of sample counts.\n",
    "        - Vm (numpy.ndarray): 3D array of mean values.\n",
    "        - Vs (numpy.ndarray): 3D array of standard deviation values.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        display(vstats)\n",
    "    src1=['Ch. 1 samples','Ch. 1 mean','Ch. 1 sdev']\n",
    "    src2=['Ch. 2 samples','Ch. 2 mean','Ch. 2 sdev']\n",
    "    Src = [src1,src2]\n",
    "    \n",
    "    Vn=[]\n",
    "    Vm=[]\n",
    "    Vs=[]\n",
    "    for src in Src:\n",
    "        #vn,vm,vs=Collect(vstats,src)\n",
    "        vn=np.asarray(vstats[src[0]],dtype=float)\n",
    "        vm=np.asarray(vstats[src[1]],dtype=float)\n",
    "        vs=np.asarray(vstats[src[2]],dtype=float)\n",
    "        Vn=np.append(Vn,vn)\n",
    "        Vm=np.append(Vm,vm)\n",
    "        Vs=np.append(Vs,vs)\n",
    "    Vn=np.reshape(Vn,(len(Src),radii.size,ctrs.size))\n",
    "    Vm=np.reshape(Vm,(len(Src),radii.size,ctrs.size))\n",
    "    Vs=np.reshape(Vs,(len(Src),radii.size,ctrs.size))\n",
    "    \n",
    "    reps=int(360/settings['Period'])\n",
    "    angle=[]\n",
    "    for i in range(reps):\n",
    "        angle=np.append(angle,ctrs+settings['Period']*i)\n",
    "    Vn=np.tile(Vn,(1,1,reps))\n",
    "    Vm=np.tile(Vm,(1,1,reps))\n",
    "    Vs=np.tile(Vs,(1,1,reps))\n",
    "    \n",
    "    Vn=np.dstack((Vn,Vn[:,:,0]))\n",
    "    Vm=np.dstack((Vm,Vm[:,:,0]))\n",
    "    Vs=np.dstack((Vs,Vs[:,:,0]))\n",
    "    angle=np.append(angle,[360])\n",
    "    angle=np.deg2rad(angle)\n",
    "    \n",
    "    theta,rad = np.meshgrid(angle,radii)\n",
    "\n",
    "    return([theta,rad,Vn,Vm,Vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotVStatsPolar(block, row, X, show=False):\n",
    "    \"\"\"\n",
    "    Plots velocity statistics on a polar plot.\n",
    "    Parameters:\n",
    "    block (tuple): A tuple containing theta, rad, Vn, Vm, Vs arrays.\n",
    "    row (dict): A dictionary containing metadata for the plot, including 'Orientation' and 'Plane'.\n",
    "    X (float): The X coordinate value for the plot.\n",
    "    show (bool, optional): If True, displays the plot. Defaults to False.\n",
    "    Returns:\n",
    "    None\n",
    "    This function generates polar plots for velocity statistics (radial, tangential, and axial velocities) \n",
    "    based on the provided data and settings. The plots are saved as PNG files in the specified output folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.close('all')\n",
    "    RadiusLimits = settings['PolarPlotRadiusLimits']\n",
    "\n",
    "    sw = 'S%04dW%04d' % (settings['Step']*100,settings['Wslot']*100)\n",
    "    outfolder=Path(settings['OutFolder'],'PolarStats',sw,'Plots')\n",
    "    outfolder.mkdir(exist_ok=True)\n",
    "    \n",
    "    theta,rad,Vn,Vm,Vs = block\n",
    "    \n",
    "    alpha=1.0\n",
    "    cmap=cm.jet\n",
    "\n",
    "    orientation = row['Orientation']\n",
    "    if 'V' in orientation:\n",
    "        Range_V1 = [settings['Vr_samp_range'], settings['Vr_mean_range'], settings['Vr_sdev_range']]\n",
    "        lbl1=[r'Count',r'$\\overline{V}_r\\ (m/s)$',r'$\\widetilde{V}_r\\ (m/s)$','Radial velocity']\n",
    "    else:\n",
    "        Range_V1 = [settings['Vt_samp_range'], settings['Vt_mean_range'], settings['Vt_sdev_range']]\n",
    "        lbl1=[r'Count',r'$\\overline{V}_t\\ (m/s)$',r'$\\widetilde{V}_t\\ (m/s)$','Tangential velocity']\n",
    "    Range_V2 = [settings['Va_samp_range'], settings['Va_mean_range'], settings['Va_sdev_range']]\n",
    "    lbl2=[r'Count',r'$\\overline{V}_a\\ (m/s)$',r'$\\widetilde{V}_a\\ (m/s)$','Axial velocity']\n",
    "    Lbl = [lbl1,lbl2]\n",
    "\n",
    "    src1=['Ch. 1 samples','Ch. 1 mean','Ch. 1 sdev',Range_V1]\n",
    "    src2=['Ch. 2 samples','Ch. 2 mean','Ch. 2 sdev',Range_V2]\n",
    "    Src = [src1,src2]\n",
    "    \n",
    "    orientation = ('Up' if orientation == 'Vu' else 'Down' if orientation == 'Vd' else 'Left' if orientation == 'Hl' else 'Right')\n",
    "    overlap = (1-settings['Step']/settings['Wslot'])*100  # overlap between adjacent slots\n",
    "\n",
    "    for k,src,lbl in list(zip(range(len(Src)),Src,Lbl)):\n",
    "\n",
    "        V=[Vn[k,:,:],Vm[k,:,:],Vs[k,:,:]]\n",
    "\n",
    "        fig = plt.figure(figsize=(18,8),facecolor='white')\n",
    "        figtitle = '%s (%s)' % (lbl[-1],orientation)\n",
    "        figtitle = figtitle + '\\n' + r'Plane %d | X=%.0f mm | Slot spacing=%0.2f$^\\circ$ | Slot width=%0.2f$^\\circ$ | Overlap=%.0f%%' % \\\n",
    "            (row['Plane'],X,settings['Step'],settings['Wslot'],overlap)\n",
    "        fig.suptitle(figtitle,fontsize=20)\n",
    "        gs=fig.add_gridspec(nrows=2,ncols=3,figure=fig,left=0.05,right=0.95,top=0.9,\n",
    "                            width_ratios=[0.3,0.3,0.3],height_ratios=[0.95,0.05])\n",
    "\n",
    "        for i,v in enumerate(V):\n",
    "\n",
    "            vma=np.ma.masked_invalid(v)\n",
    "            cond= (rad<RadiusLimits[0]) | (rad>RadiusLimits[1])\n",
    "            vma=np.ma.masked_array(vma,mask=cond)\n",
    "            \n",
    "            vmin=src[-1][i][0]\n",
    "            vmax=src[-1][i][1]\n",
    "            if settings['Autoscale']:\n",
    "                if i==1:\n",
    "                    vmin=np.mean(vma)-2*np.std(vma)\n",
    "                else:\n",
    "                    vmin=np.min(vma)\n",
    "                vmax=np.mean(vma)+2*np.std(vma)\n",
    "            #print(vmin,vmax)\n",
    "\n",
    "            fact=1.0\n",
    "            if lbl[-1]=='Axial velocity':\n",
    "                fact=settings['RefractiveIndexCorrection']\n",
    "            norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "            ax=fig.add_subplot(gs[0,i],projection='polar')\n",
    "            ax.set_facecolor('white')\n",
    "            ax.grid(False)\n",
    "            ax.pcolormesh(theta,rad*fact,vma,cmap=cmap,norm=norm,\n",
    "                          edgecolors='none',linewidth=0.1,\n",
    "                          shading='gouraud',alpha=alpha)\n",
    "            ax.set_xlabel(lbl[i],size=20)\n",
    "            #ax.set_xlim(0,2*np.pi)\n",
    "            ax.set_ylim(0,1.25)\n",
    "            ax.grid(which='both')\n",
    "            cax=plt.subplot(gs[1,i])\n",
    "            ColorbarBase(ax=cax,cmap=cmap,norm=norm,orientation='horizontal')\n",
    "\n",
    "            #break\n",
    "\n",
    "        outfile='%s_Stats_%s_%s%s_P%02d' % (settings['Case'],sw,\n",
    "                                            lbl[-1].split(' ')[0],orientation,\n",
    "                                            row['Plane'])\n",
    "        plt.savefig(Path(outfolder,outfile).with_suffix('.png'),\n",
    "                    facecolor=fig.get_facecolor(),dpi=300)\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolarPlots(verbose=False, show=False):\n",
    "    \"\"\"\n",
    "    Generate polar plots for LDV measurements.\n",
    "    Parameters:\n",
    "    verbose (bool): If True, print detailed information and display data.\n",
    "    show (bool): If True, display the plots.\n",
    "    This function reads measurement data from a feather file, processes it, and generates polar plots.\n",
    "    It checks for the existence of required data files and prints missing files if any.\n",
    "    The function iterates over specified planes and orientations, processes the data, and generates plots.\n",
    "    It also exports the processed data to VTK format.\n",
    "    The function uses the following settings from a global `settings` dictionary:\n",
    "    - 'OutFolder': Output folder path.\n",
    "    - 'Case': Case identifier.\n",
    "    - 'Step': Step size.\n",
    "    - 'Wslot': Slot width.\n",
    "    - 'PlaneRange': Range of planes to process.\n",
    "    - 'Rref': Reference radius.\n",
    "    - 'Period': Period for interval setting.\n",
    "    - 'Wleft': Left width for interval setting.\n",
    "    - 'Wright': Right width for interval setting.\n",
    "    The function uses the following external functions:\n",
    "    - SetIntervals: To set intervals for data processing.\n",
    "    - BuildBlocks: To build data blocks for plotting.\n",
    "    - PlotVStatsPolar: To plot the polar statistics.\n",
    "    - ExportToVTKVtu: To export data to VTK format.\n",
    "    \"\"\"\n",
    "    \n",
    "    DataPath=Path(settings['OutFolder'],'%s_Stats4.fth' % settings['Case'])\n",
    "    if not DataPath.exists():\n",
    "        print('%s does not exist' % DataPath)\n",
    "        exit()\n",
    "\n",
    "    Data=pd.read_feather(DataPath)\n",
    "    if verbose:\n",
    "        display(Data)\n",
    "\n",
    "    sw = 'S%04dW%04d' % (settings['Step']*100,settings['Wslot']*100)\n",
    "    datafolder=Path(settings['OutFolder'],'PolarStats',sw,'Csv')\n",
    "    Statfile=[item for item in datafolder.iterdir()]\n",
    "    if verbose:\n",
    "        print(Statfile)\n",
    "\n",
    "    Planes = settings['PlaneRange']\n",
    "    if Planes==[-1]:\n",
    "        seen = set()\n",
    "        Planes = []\n",
    "        for x in Data['Plane']:\n",
    "            if x not in seen:\n",
    "                Planes.append(x)\n",
    "                seen.add(x)\n",
    "    \n",
    "        Planes.sort()\n",
    "\n",
    "    print(\"%d planes:\" % len(Planes), Planes)\n",
    "    \n",
    "    with tqdm(total=len(Planes),dynamic_ncols=True,desc=\"Plane P%02d\" % Planes[0]) as pbar:\n",
    "        for plane in Planes:\n",
    "            cond0=Data['Plane']==plane\n",
    "\n",
    "            pbar.desc=\"P%02d\" % plane\n",
    "            pbar.update()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            for orientation in ['Vu','Vd','Hl','Hr']:\n",
    "                cond1=Data['Orientation']==orientation\n",
    "                data=Data[cond0 & cond1]\n",
    "\n",
    "                if len(data)==0:\n",
    "                    continue\n",
    "                \n",
    "                count=0\n",
    "                for _,row in data.iterrows():\n",
    "                    statfile=Path(datafolder, '%s_Stats_%s_P%06d' \\\n",
    "                        % (settings['Case'],sw,row['Point'])).with_suffix('.csv')\n",
    "\n",
    "                    if statfile in Statfile:\n",
    "                        count+=1\n",
    "                    else:\n",
    "                        print('Missing %s' % statfile)\n",
    "                if count<len(data):\n",
    "                    print('Incomplete data for plane %d (%s)' % (plane,orientation))\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print('Data:',len(data), 'Count:',count)\n",
    "\n",
    "                R = data['R (mm)']/settings['Rref']\n",
    "                R = np.sort(R)\n",
    "                X = np.mean(data.loc[data['Plane']==plane,'X (mm)'])\n",
    "\n",
    "                #print('R:',len(R),data['Orientation'],data['Point'],len(data))\n",
    "                _,Ctrs=SetIntervals(settings['Period'],settings['Step'],\n",
    "                                            settings['Wleft'],settings['Wright'])\n",
    "\n",
    "                vstats=pd.DataFrame()\n",
    "                for _,row in data.iterrows():\n",
    "                    plane = row['Plane']\n",
    "\n",
    "                    statfile=Path(datafolder, '%s_Stats_%s_P%06d' \\\n",
    "                        % (settings['Case'],sw,row['Point'])).with_suffix('.csv')\n",
    "                    vstat=pd.read_csv(statfile)\n",
    "                    if len(vstats)==0:\n",
    "                        vstats=vstat\n",
    "                    else:   \n",
    "                        vstats=pd.concat([vstats,vstat],ignore_index=True)\n",
    "\n",
    "                    #if verbose:\n",
    "                    #    display(vstats)\n",
    "                #print(vstats.shape)\n",
    "                Block = BuildBlocks(R,Ctrs,vstats,verbose=False)\n",
    "                PlotVStatsPolar(Block,row,X,show)\n",
    "                ExportToVTKVtu(Block,row,X)\n",
    "\n",
    "            if verbose:\n",
    "                print('Plane %02d: %f seconds' % (plane, timeit.default_timer() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Phase analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PhaseAnalysis(verbose=False, show=False):\n",
    "    \"\"\"\n",
    "    Perform phase analysis on measurement data.\n",
    "    Parameters:\n",
    "    verbose (bool): If True, display the original and filtered data. Default is False.\n",
    "    show (bool): If True, show the results of the single point analysis. Default is False.\n",
    "    Returns:\n",
    "    None\n",
    "    This function reads measurement data from a feather file, filters it based on specified \n",
    "    conditions, and performs single point analysis on the filtered data. The progress of the \n",
    "    analysis is displayed using a progress bar. If the verbose parameter is set to True, the \n",
    "    original and filtered data are displayed. If the show parameter is set to True, the results \n",
    "    of the single point analysis are shown.\n",
    "    \"\"\"\n",
    "\n",
    "    DataPath=Path(settings['OutFolder'],'%s_Stats4.fth' % settings['Case'])\n",
    "    if not DataPath.exists():\n",
    "        print('%s does not exist' % DataPath)\n",
    "        exit()\n",
    "\n",
    "    Data=pd.read_feather(DataPath)\n",
    "    if verbose:\n",
    "        display(Data)\n",
    "\n",
    "    Planes = settings['PlaneRange']\n",
    "    if Planes==[-1]:\n",
    "        seen = set()\n",
    "        Planes = []\n",
    "        for x in Data['Plane']:\n",
    "            if x not in seen:\n",
    "                Planes.append(x)\n",
    "                seen.add(x)\n",
    "    \n",
    "        Planes.sort()\n",
    "\n",
    "    print(\"%d planes:\" % len(Planes), Planes)\n",
    "\n",
    "    cond0 = Data['Plane'].apply(lambda x: x in Planes)\n",
    "    cond1 = Data['R (mm)'].apply(lambda x: x >= settings['RadiusRange'][0])\n",
    "    cond2 = Data['R (mm)'].apply(lambda x: x <= settings['RadiusRange'][-1])\n",
    "    data = Data[cond0 & cond1 & cond2]\n",
    "    if len(data)==0:\n",
    "        print('No data for phase analysis')\n",
    "        return\n",
    "    \n",
    "    if verbose:\n",
    "        display(Data)\n",
    "        display(data) \n",
    "\n",
    "    with tqdm(total=len(data),dynamic_ncols=True,desc=\"%s P%02d\" % \\\n",
    "        (data.loc[data.index[0],'File'],data.loc[data.index[0],'Plane'])) as pbar:\n",
    "        for _,row in data.iterrows():\n",
    "            file = row['File']\n",
    "            plane = row['Plane']\n",
    "            \n",
    "            pbar.desc=\"%s P%02d\" % (file, plane)\n",
    "            pbar.update()\n",
    "    \n",
    "            if verbose:\n",
    "                print('\\r%s Plane %02d: ' % (file, plane), end='')\n",
    "            start_time = timeit.default_timer()\n",
    "           \n",
    "            n = SinglePointAnalysis(row,show) \n",
    "   \n",
    "            if verbose:\n",
    "                dt = timeit.default_timer() - start_time\n",
    "                print('%d points, %f seconds\\n' % (n, dt), end='')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "Main"
    ]
   },
   "outputs": [],
   "source": [
    "def GenerateDatabase(verbose=False):\n",
    "    LoadStatFiles(verbose)\n",
    "    FindPlanes(verbose)\n",
    "    FindRepeats(verbose)\n",
    "    FindMatches(verbose)\n",
    "    ComputeStats(verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating database\n",
      "Data/Exports/CRP1_LDV/unified-data unified-data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('Data/Exports/CRP1_LDV/unified-data/B00')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('Data/Exports/CRP1_LDV/unified-data/B00/B00.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(Files): 2575\n",
      "ic| len(data): 2575\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X Axis Position</th>\n",
       "      <th>Y Axis Position</th>\n",
       "      <th>Z Axis Position</th>\n",
       "      <th>Velocity Mean Ch. 1 (m/sec)</th>\n",
       "      <th>Velocity Mean Ch. 2 (m/sec)</th>\n",
       "      <th>Velocity RMS Ch. 1 (m/sec)</th>\n",
       "      <th>Velocity RMS Ch. 2 (m/sec)</th>\n",
       "      <th>Velocity Skewness Ch. 1 ()</th>\n",
       "      <th>Velocity Skewness Ch. 2 ()</th>\n",
       "      <th>Velocity Flatness Ch. 1 ()</th>\n",
       "      <th>Velocity Flatness Ch. 2 ()</th>\n",
       "      <th>Vel.Data Rate Ch. 1 (Hz)</th>\n",
       "      <th>Vel.Data Rate Ch. 2 (Hz)</th>\n",
       "      <th>Turbulence Intensity Ch. 1 (%)</th>\n",
       "      <th>Turbulence Intensity Ch. 2 (%)</th>\n",
       "      <th>Valid Vel.Count Ch. 1</th>\n",
       "      <th>Valid Vel.Count Ch. 2</th>\n",
       "      <th>Frequency Mean Ch. 1 (MHz)</th>\n",
       "      <th>Frequency Mean Ch. 2 (MHz)</th>\n",
       "      <th>Analog Extern Ch. 1 Mean</th>\n",
       "      <th>Analog Extern Ch. 1 RMS</th>\n",
       "      <th>Analog Extern Ch. 2 Mean</th>\n",
       "      <th>Analog Extern Ch. 2 RMS</th>\n",
       "      <th>Analog Extern Ch. 3 Mean</th>\n",
       "      <th>Analog Extern Ch. 3 RMS</th>\n",
       "      <th>Analog Extern Ch. 4 Mean</th>\n",
       "      <th>Analog Extern Ch. 4 RMS</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Block</th>\n",
       "      <th>File</th>\n",
       "      <th>Point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.600000e+01</td>\n",
       "      <td>2.467000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.291735e+00</td>\n",
       "      <td>-6.752531e+00</td>\n",
       "      <td>8.606634e-01</td>\n",
       "      <td>1.051985e+00</td>\n",
       "      <td>-5.061086e-01</td>\n",
       "      <td>9.205769e-01</td>\n",
       "      <td>4.021660e+00</td>\n",
       "      <td>4.877761e+00</td>\n",
       "      <td>192</td>\n",
       "      <td>252</td>\n",
       "      <td>6.662846e+01</td>\n",
       "      <td>1.557913e+01</td>\n",
       "      <td>36795</td>\n",
       "      <td>48439</td>\n",
       "      <td>1.283596e+00</td>\n",
       "      <td>1.455833e+00</td>\n",
       "      <td>1.845683e+02</td>\n",
       "      <td>9.901582e+01</td>\n",
       "      <td>1.795431e+01</td>\n",
       "      <td>9.646875e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-000001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.600000e+01</td>\n",
       "      <td>2.878000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.494707e-01</td>\n",
       "      <td>-7.873198e+00</td>\n",
       "      <td>6.318594e-01</td>\n",
       "      <td>6.253873e-01</td>\n",
       "      <td>-1.637119e+00</td>\n",
       "      <td>2.149937e+00</td>\n",
       "      <td>8.824883e+00</td>\n",
       "      <td>1.150403e+01</td>\n",
       "      <td>1876</td>\n",
       "      <td>1992</td>\n",
       "      <td>1.149942e+02</td>\n",
       "      <td>7.943244e+00</td>\n",
       "      <td>116391</td>\n",
       "      <td>123609</td>\n",
       "      <td>1.465417e+00</td>\n",
       "      <td>1.166354e+00</td>\n",
       "      <td>1.843214e+02</td>\n",
       "      <td>9.863808e+01</td>\n",
       "      <td>1.793090e+01</td>\n",
       "      <td>9.609397e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-000002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.600000e+01</td>\n",
       "      <td>3.289000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.658157e-01</td>\n",
       "      <td>-8.119818e+00</td>\n",
       "      <td>6.018495e-01</td>\n",
       "      <td>6.057615e-01</td>\n",
       "      <td>-2.624980e+00</td>\n",
       "      <td>2.908671e+00</td>\n",
       "      <td>1.531614e+01</td>\n",
       "      <td>1.676341e+01</td>\n",
       "      <td>2893</td>\n",
       "      <td>2518</td>\n",
       "      <td>1.645226e+02</td>\n",
       "      <td>7.460284e+00</td>\n",
       "      <td>128297</td>\n",
       "      <td>111703</td>\n",
       "      <td>1.510391e+00</td>\n",
       "      <td>1.102644e+00</td>\n",
       "      <td>1.840450e+02</td>\n",
       "      <td>9.886449e+01</td>\n",
       "      <td>1.790450e+01</td>\n",
       "      <td>9.630553e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-000003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.600000e+01</td>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.649789e-01</td>\n",
       "      <td>-8.253921e+00</td>\n",
       "      <td>5.978552e-01</td>\n",
       "      <td>6.245264e-01</td>\n",
       "      <td>-2.614737e+00</td>\n",
       "      <td>3.561874e+00</td>\n",
       "      <td>1.746107e+01</td>\n",
       "      <td>2.274365e+01</td>\n",
       "      <td>2697</td>\n",
       "      <td>2708</td>\n",
       "      <td>3.623827e+02</td>\n",
       "      <td>7.566422e+00</td>\n",
       "      <td>119766</td>\n",
       "      <td>120234</td>\n",
       "      <td>1.559596e+00</td>\n",
       "      <td>1.068025e+00</td>\n",
       "      <td>1.843508e+02</td>\n",
       "      <td>9.862804e+01</td>\n",
       "      <td>1.793441e+01</td>\n",
       "      <td>9.608060e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-000004</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.600000e+01</td>\n",
       "      <td>4.111000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.406332e-02</td>\n",
       "      <td>-8.336810e+00</td>\n",
       "      <td>6.185467e-01</td>\n",
       "      <td>6.640166e-01</td>\n",
       "      <td>-2.122459e+00</td>\n",
       "      <td>3.911446e+00</td>\n",
       "      <td>1.571181e+01</td>\n",
       "      <td>2.829460e+01</td>\n",
       "      <td>2962</td>\n",
       "      <td>2831</td>\n",
       "      <td>1.144115e+03</td>\n",
       "      <td>7.964876e+00</td>\n",
       "      <td>122715</td>\n",
       "      <td>117285</td>\n",
       "      <td>1.586766e+00</td>\n",
       "      <td>1.046580e+00</td>\n",
       "      <td>1.842950e+02</td>\n",
       "      <td>9.871932e+01</td>\n",
       "      <td>1.792926e+01</td>\n",
       "      <td>9.617048e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-000005</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>-2.610100e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.093600e+02</td>\n",
       "      <td>-1.076148e-01</td>\n",
       "      <td>-5.934408e+00</td>\n",
       "      <td>9.136416e-01</td>\n",
       "      <td>6.056679e-01</td>\n",
       "      <td>7.528369e-01</td>\n",
       "      <td>-1.159672e+00</td>\n",
       "      <td>6.207444e+00</td>\n",
       "      <td>1.115972e+01</td>\n",
       "      <td>2644</td>\n",
       "      <td>2511</td>\n",
       "      <td>8.489922e+02</td>\n",
       "      <td>1.020604e+01</td>\n",
       "      <td>307770</td>\n",
       "      <td>292230</td>\n",
       "      <td>1.573645e+00</td>\n",
       "      <td>1.667163e+00</td>\n",
       "      <td>1.841352e+02</td>\n",
       "      <td>9.830302e+01</td>\n",
       "      <td>1.791590e+01</td>\n",
       "      <td>9.576706e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2571</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-002571</td>\n",
       "      <td>2571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>-2.610100e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.148300e+02</td>\n",
       "      <td>-1.284427e-01</td>\n",
       "      <td>-5.735847e+00</td>\n",
       "      <td>6.239062e-01</td>\n",
       "      <td>3.990207e-01</td>\n",
       "      <td>7.380329e-01</td>\n",
       "      <td>1.239571e+00</td>\n",
       "      <td>6.925681e+00</td>\n",
       "      <td>9.021661e+00</td>\n",
       "      <td>1944</td>\n",
       "      <td>2546</td>\n",
       "      <td>4.857466e+02</td>\n",
       "      <td>6.956613e+00</td>\n",
       "      <td>231055</td>\n",
       "      <td>302555</td>\n",
       "      <td>1.568549e+00</td>\n",
       "      <td>1.718423e+00</td>\n",
       "      <td>1.844856e+02</td>\n",
       "      <td>9.837354e+01</td>\n",
       "      <td>1.795039e+01</td>\n",
       "      <td>9.583693e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2572</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-002572</td>\n",
       "      <td>2572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>-2.610100e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.203000e+02</td>\n",
       "      <td>-1.544225e-01</td>\n",
       "      <td>-5.681212e+00</td>\n",
       "      <td>4.251833e-01</td>\n",
       "      <td>2.883469e-01</td>\n",
       "      <td>6.005425e-02</td>\n",
       "      <td>1.067231e+00</td>\n",
       "      <td>1.334602e+01</td>\n",
       "      <td>5.755093e+00</td>\n",
       "      <td>30</td>\n",
       "      <td>2683</td>\n",
       "      <td>2.753377e+02</td>\n",
       "      <td>5.075448e+00</td>\n",
       "      <td>3610</td>\n",
       "      <td>319220</td>\n",
       "      <td>1.562176e+00</td>\n",
       "      <td>1.732531e+00</td>\n",
       "      <td>1.843217e+02</td>\n",
       "      <td>9.841017e+01</td>\n",
       "      <td>1.793471e+01</td>\n",
       "      <td>9.586961e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2573</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-002573</td>\n",
       "      <td>2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>-2.610100e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.257600e+02</td>\n",
       "      <td>-1.131462e+00</td>\n",
       "      <td>-5.673021e+00</td>\n",
       "      <td>1.461042e+00</td>\n",
       "      <td>1.975942e-01</td>\n",
       "      <td>-4.889502e-01</td>\n",
       "      <td>5.429118e-01</td>\n",
       "      <td>1.910393e+00</td>\n",
       "      <td>4.255741e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>2375</td>\n",
       "      <td>1.291286e+02</td>\n",
       "      <td>3.483050e+00</td>\n",
       "      <td>20</td>\n",
       "      <td>282940</td>\n",
       "      <td>1.322852e+00</td>\n",
       "      <td>1.734659e+00</td>\n",
       "      <td>1.841012e+02</td>\n",
       "      <td>9.837655e+01</td>\n",
       "      <td>1.791257e+01</td>\n",
       "      <td>9.584385e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2574</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-002574</td>\n",
       "      <td>2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>-2.610100e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.285000e+02</td>\n",
       "      <td>-1.657653e+00</td>\n",
       "      <td>-5.674434e+00</td>\n",
       "      <td>2.469560e+00</td>\n",
       "      <td>1.667280e-01</td>\n",
       "      <td>8.194221e-01</td>\n",
       "      <td>3.455289e-01</td>\n",
       "      <td>2.003493e+00</td>\n",
       "      <td>4.117162e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>2560</td>\n",
       "      <td>1.489793e+02</td>\n",
       "      <td>2.938232e+00</td>\n",
       "      <td>4</td>\n",
       "      <td>305366</td>\n",
       "      <td>1.193963e+00</td>\n",
       "      <td>1.734287e+00</td>\n",
       "      <td>1.840722e+02</td>\n",
       "      <td>9.826479e+01</td>\n",
       "      <td>1.791058e+01</td>\n",
       "      <td>9.573050e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2575</td>\n",
       "      <td>B00</td>\n",
       "      <td>Unified-data-002575</td>\n",
       "      <td>2575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2575 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X Axis Position  Y Axis Position  Z Axis Position  Velocity Mean Ch. 1 (m/sec)  Velocity Mean Ch. 2 (m/sec)  Velocity RMS Ch. 1 (m/sec)  Velocity RMS Ch. 2 (m/sec)  Velocity Skewness Ch. 1 ()  Velocity Skewness Ch. 2 ()  Velocity Flatness Ch. 1 ()  Velocity Flatness Ch. 2 ()  Vel.Data Rate Ch. 1 (Hz)  Vel.Data Rate Ch. 2 (Hz)  Turbulence Intensity Ch. 1 (%)  Turbulence Intensity Ch. 2 (%)  Valid Vel.Count Ch. 1  Valid Vel.Count Ch. 2  Frequency Mean Ch. 1 (MHz)  Frequency Mean Ch. 2 (MHz)  Analog Extern Ch. 1 Mean  Analog Extern Ch. 1 RMS  Analog Extern Ch. 2 Mean  Analog Extern Ch. 2 RMS  Analog Extern Ch. 3 Mean  Analog Extern Ch. 3 RMS  Analog Extern Ch. 4 Mean  Analog Extern Ch. 4 RMS  Sequence Number Block                 File  Point\n",
       "0       -8.600000e+01     2.467000e+01     0.000000e+00                -1.291735e+00                -6.752531e+00                8.606634e-01                1.051985e+00               -5.061086e-01                9.205769e-01                4.021660e+00                4.877761e+00                       192                       252                    6.662846e+01                    1.557913e+01                  36795                  48439                1.283596e+00                1.455833e+00              1.845683e+02             9.901582e+01              1.795431e+01             9.646875e+00                         0                        0                         0                        0                1   B00  Unified-data-000001      1\n",
       "1       -8.600000e+01     2.878000e+01     0.000000e+00                -5.494707e-01                -7.873198e+00                6.318594e-01                6.253873e-01               -1.637119e+00                2.149937e+00                8.824883e+00                1.150403e+01                      1876                      1992                    1.149942e+02                    7.943244e+00                 116391                 123609                1.465417e+00                1.166354e+00              1.843214e+02             9.863808e+01              1.793090e+01             9.609397e+00                         0                        0                         0                        0                2   B00  Unified-data-000002      2\n",
       "2       -8.600000e+01     3.289000e+01     0.000000e+00                -3.658157e-01                -8.119818e+00                6.018495e-01                6.057615e-01               -2.624980e+00                2.908671e+00                1.531614e+01                1.676341e+01                      2893                      2518                    1.645226e+02                    7.460284e+00                 128297                 111703                1.510391e+00                1.102644e+00              1.840450e+02             9.886449e+01              1.790450e+01             9.630553e+00                         0                        0                         0                        0                3   B00  Unified-data-000003      3\n",
       "3       -8.600000e+01     3.700000e+01     0.000000e+00                -1.649789e-01                -8.253921e+00                5.978552e-01                6.245264e-01               -2.614737e+00                3.561874e+00                1.746107e+01                2.274365e+01                      2697                      2708                    3.623827e+02                    7.566422e+00                 119766                 120234                1.559596e+00                1.068025e+00              1.843508e+02             9.862804e+01              1.793441e+01             9.608060e+00                         0                        0                         0                        0                4   B00  Unified-data-000004      4\n",
       "4       -8.600000e+01     4.111000e+01     0.000000e+00                -5.406332e-02                -8.336810e+00                6.185467e-01                6.640166e-01               -2.122459e+00                3.911446e+00                1.571181e+01                2.829460e+01                      2962                      2831                    1.144115e+03                    7.964876e+00                 122715                 117285                1.586766e+00                1.046580e+00              1.842950e+02             9.871932e+01              1.792926e+01             9.617048e+00                         0                        0                         0                        0                5   B00  Unified-data-000005      5\n",
       "...               ...              ...              ...                          ...                          ...                         ...                         ...                         ...                         ...                         ...                         ...                       ...                       ...                             ...                             ...                    ...                    ...                         ...                         ...                       ...                      ...                       ...                      ...                       ...                      ...                       ...                      ...              ...   ...                  ...    ...\n",
       "2570    -2.610100e+02     0.000000e+00    -1.093600e+02                -1.076148e-01                -5.934408e+00                9.136416e-01                6.056679e-01                7.528369e-01               -1.159672e+00                6.207444e+00                1.115972e+01                      2644                      2511                    8.489922e+02                    1.020604e+01                 307770                 292230                1.573645e+00                1.667163e+00              1.841352e+02             9.830302e+01              1.791590e+01             9.576706e+00                         0                        0                         0                        0             2571   B00  Unified-data-002571   2571\n",
       "2571    -2.610100e+02     0.000000e+00    -1.148300e+02                -1.284427e-01                -5.735847e+00                6.239062e-01                3.990207e-01                7.380329e-01                1.239571e+00                6.925681e+00                9.021661e+00                      1944                      2546                    4.857466e+02                    6.956613e+00                 231055                 302555                1.568549e+00                1.718423e+00              1.844856e+02             9.837354e+01              1.795039e+01             9.583693e+00                         0                        0                         0                        0             2572   B00  Unified-data-002572   2572\n",
       "2572    -2.610100e+02     0.000000e+00    -1.203000e+02                -1.544225e-01                -5.681212e+00                4.251833e-01                2.883469e-01                6.005425e-02                1.067231e+00                1.334602e+01                5.755093e+00                        30                      2683                    2.753377e+02                    5.075448e+00                   3610                 319220                1.562176e+00                1.732531e+00              1.843217e+02             9.841017e+01              1.793471e+01             9.586961e+00                         0                        0                         0                        0             2573   B00  Unified-data-002573   2573\n",
       "2573    -2.610100e+02     0.000000e+00    -1.257600e+02                -1.131462e+00                -5.673021e+00                1.461042e+00                1.975942e-01               -4.889502e-01                5.429118e-01                1.910393e+00                4.255741e+00                         0                      2375                    1.291286e+02                    3.483050e+00                     20                 282940                1.322852e+00                1.734659e+00              1.841012e+02             9.837655e+01              1.791257e+01             9.584385e+00                         0                        0                         0                        0             2574   B00  Unified-data-002574   2574\n",
       "2574    -2.610100e+02     0.000000e+00    -1.285000e+02                -1.657653e+00                -5.674434e+00                2.469560e+00                1.667280e-01                8.194221e-01                3.455289e-01                2.003493e+00                4.117162e+00                         0                      2560                    1.489793e+02                    2.938232e+00                      4                 305366                1.193963e+00                1.734287e+00              1.840722e+02             9.826479e+01              1.791058e+01             9.573050e+00                         0                        0                         0                        0             2575   B00  Unified-data-002575   2575\n",
       "\n",
       "[2575 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2575 entries, 0 to 2574\n",
      "Data columns (total 31 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   X Axis Position                 2575 non-null   float64\n",
      " 1   Y Axis Position                 2575 non-null   float64\n",
      " 2   Z Axis Position                 2575 non-null   float64\n",
      " 3   Velocity Mean Ch. 1 (m/sec)     2575 non-null   float64\n",
      " 4   Velocity Mean Ch. 2 (m/sec)     2575 non-null   float64\n",
      " 5   Velocity RMS Ch. 1 (m/sec)      2575 non-null   float64\n",
      " 6   Velocity RMS Ch. 2 (m/sec)      2575 non-null   float64\n",
      " 7   Velocity Skewness Ch. 1 ()      2575 non-null   float64\n",
      " 8   Velocity Skewness Ch. 2 ()      2575 non-null   float64\n",
      " 9   Velocity Flatness Ch. 1 ()      2575 non-null   float64\n",
      " 10  Velocity Flatness Ch. 2 ()      2575 non-null   float64\n",
      " 11  Vel.Data Rate Ch. 1 (Hz)        2575 non-null   int64  \n",
      " 12  Vel.Data Rate Ch. 2 (Hz)        2575 non-null   int64  \n",
      " 13  Turbulence Intensity Ch. 1 (%)  2575 non-null   float64\n",
      " 14  Turbulence Intensity Ch. 2 (%)  2575 non-null   float64\n",
      " 15  Valid Vel.Count Ch. 1           2575 non-null   int64  \n",
      " 16  Valid Vel.Count Ch. 2           2575 non-null   int64  \n",
      " 17  Frequency Mean Ch. 1 (MHz)      2575 non-null   float64\n",
      " 18  Frequency Mean Ch. 2 (MHz)      2575 non-null   float64\n",
      " 19  Analog Extern Ch. 1 Mean        2575 non-null   float64\n",
      " 20  Analog Extern Ch. 1 RMS         2575 non-null   float64\n",
      " 21  Analog Extern Ch. 2 Mean        2575 non-null   float64\n",
      " 22  Analog Extern Ch. 2 RMS         2575 non-null   float64\n",
      " 23  Analog Extern Ch. 3 Mean        2575 non-null   int64  \n",
      " 24  Analog Extern Ch. 3 RMS         2575 non-null   int64  \n",
      " 25  Analog Extern Ch. 4 Mean        2575 non-null   int64  \n",
      " 26  Analog Extern Ch. 4 RMS         2575 non-null   int64  \n",
      " 27  Sequence Number                 2575 non-null   int64  \n",
      " 28  Block                           2575 non-null   object \n",
      " 29  File                            2575 non-null   object \n",
      " 30  Point                           2575 non-null   int64  \n",
      "dtypes: float64(19), int64(10), object(2)\n",
      "memory usage: 623.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 planes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac27144ea2d4ec78ac9bc44f1c01559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unified-data-001359:   0%|          | 0/2575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CH1 Unified-data-001359.csv: in 600, out 600\n",
      "CH2 Unified-data-001359.csv: in 3, out 3\n",
      "CH1 Unified-data-001360.csv: in 28073, out 28073\n",
      "CH2 Unified-data-001360.csv: in 1, out 1\n",
      "CH1 Unified-data-001361.csv: in 599840, out 599840\n",
      "CH2 Unified-data-001361.csv: in 160, out 160\n",
      "CH1 Unified-data-001362.csv: in 58196, out 58196\n",
      "CH2 Unified-data-001362.csv: in 32878, out 32878\n",
      "CH1 Unified-data-001363.csv: in 182500, out 182500\n",
      "CH2 Unified-data-001363.csv: in 131510, out 131510\n",
      "CH1 Unified-data-001364.csv: in 326736, out 326736\n",
      "CH2 Unified-data-001364.csv: in 273264, out 273264\n",
      "CH1 Unified-data-001365.csv: in 316393, out 316393\n",
      "CH2 Unified-data-001365.csv: in 283607, out 283607\n",
      "CH1 Unified-data-001366.csv: in 286533, out 286533\n",
      "CH2 Unified-data-001366.csv: in 313467, out 313467\n",
      "CH1 Unified-data-001367.csv: in 283839, out 283839\n",
      "CH2 Unified-data-001367.csv: in 316161, out 316161\n",
      "CH1 Unified-data-001368.csv: in 293166, out 293166\n",
      "CH2 Unified-data-001368.csv: in 306834, out 306834\n",
      "CH1 Unified-data-001369.csv: in 300559, out 300559\n",
      "CH2 Unified-data-001369.csv: in 299441, out 299441\n",
      "CH1 Unified-data-001370.csv: in 258758, out 258758\n",
      "CH2 Unified-data-001370.csv: in 341242, out 341242\n",
      "CH1 Unified-data-001371.csv: in 275764, out 275764\n",
      "CH2 Unified-data-001371.csv: in 324236, out 324236\n",
      "CH1 Unified-data-001372.csv: in 202622, out 202622\n",
      "CH2 Unified-data-001372.csv: in 397378, out 397378\n",
      "CH1 Unified-data-001373.csv: in 251942, out 251942\n",
      "CH2 Unified-data-001373.csv: in 348058, out 348058\n",
      "CH1 Unified-data-001374.csv: in 198648, out 198648\n",
      "CH2 Unified-data-001374.csv: in 401352, out 401352\n",
      "CH1 Unified-data-001375.csv: in 277212, out 277212\n",
      "CH2 Unified-data-001375.csv: in 322788, out 322788\n",
      "CH1 Unified-data-001376.csv: in 274099, out 274099\n",
      "CH2 Unified-data-001376.csv: in 325901, out 325901\n",
      "CH1 Unified-data-001377.csv: in 285701, out 285701\n",
      "CH2 Unified-data-001377.csv: in 314299, out 314299\n",
      "CH1 Unified-data-001378.csv: in 290569, out 290569\n",
      "CH2 Unified-data-001378.csv: in 309431, out 309431\n",
      "CH1 Unified-data-001379.csv: in 275125, out 275125\n",
      "CH2 Unified-data-001379.csv: in 324875, out 324875\n",
      "CH1 Unified-data-001380.csv: in 285976, out 285976\n",
      "CH2 Unified-data-001380.csv: in 314024, out 314024\n",
      "CH1 Unified-data-001381.csv: in 78476, out 78476\n",
      "CH2 Unified-data-001381.csv: in 112146, out 112146\n",
      "CH1 Unified-data-001382.csv: in 271065, out 271065\n",
      "CH2 Unified-data-001382.csv: in 328935, out 328935\n",
      "CH1 Unified-data-001383.csv: in 300348, out 300348\n",
      "CH2 Unified-data-001383.csv: in 299652, out 299652\n",
      "CH1 Unified-data-001384.csv: in 415090, out 415090\n",
      "CH2 Unified-data-001384.csv: in 137240, out 137240\n",
      "CH1 Unified-data-001385.csv: in 432021, out 432021\n",
      "CH2 Unified-data-001385.csv: in 167979, out 167979\n",
      "CH1 Unified-data-001386.csv: in 309124, out 309124\n",
      "CH2 Unified-data-001386.csv: in 290876, out 290876\n",
      "CH1 Unified-data-001387.csv: in 333712, out 333712\n",
      "CH2 Unified-data-001387.csv: in 266288, out 266288\n",
      "CH1 Unified-data-001388.csv: in 358199, out 358199\n",
      "CH2 Unified-data-001388.csv: in 164701, out 164701\n",
      "CH1 Unified-data-001389.csv: in 234492, out 234492\n",
      "CH2 Unified-data-001389.csv: in 357348, out 357348\n",
      "CH1 Unified-data-001390.csv: in 326131, out 326131\n",
      "CH2 Unified-data-001390.csv: in 273869, out 273869\n",
      "CH1 Unified-data-001391.csv: in 338777, out 338777\n",
      ">>> Invalid value in Unified-data-001391.csv CH2: in 261223, out 261222 (1)\n",
      "CH1 Unified-data-001392.csv: in 239399, out 239399\n",
      "CH2 Unified-data-001392.csv: in 254071, out 254071\n",
      "CH1 Unified-data-001393.csv: in 180551, out 180551\n",
      "CH2 Unified-data-001393.csv: in 171259, out 171259\n",
      "CH1 Unified-data-001394.csv: in 180342, out 180342\n",
      "CH2 Unified-data-001394.csv: in 193518, out 193518\n",
      "CH1 Unified-data-001395.csv: in 186682, out 186682\n",
      "CH2 Unified-data-001395.csv: in 177638, out 177638\n",
      "CH1 Unified-data-001396.csv: in 129468, out 129468\n",
      "CH2 Unified-data-001396.csv: in 142422, out 142422\n",
      "CH1 Unified-data-001397.csv: in 153069, out 153069\n",
      "CH2 Unified-data-001397.csv: in 114771, out 114771\n",
      "CH1 Unified-data-001398.csv: in 181817, out 181817\n",
      "CH2 Unified-data-001398.csv: in 149383, out 149383\n",
      "CH1 Unified-data-001399.csv: in 6019, out 6019\n",
      "CH2 Unified-data-001399.csv: in 299081, out 299081\n",
      "CH1 Unified-data-001400.csv: in 5, out 5\n",
      "CH2 Unified-data-001400.csv: in 234445, out 234445\n",
      "CH1 Unified-data-001401.csv: in 6, out 6\n",
      "CH2 Unified-data-001401.csv: in 285924, out 285924\n",
      "CH1 Unified-data-001402.csv: in 109805, out 109805\n",
      "CH2 Unified-data-001402.csv: in 527, out 527\n",
      "CH1 Unified-data-001403.csv: in 80161, out 80161\n",
      "CH2 Unified-data-001403.csv: in 34769, out 34769\n",
      "CH1 Unified-data-001404.csv: in 177388, out 177388\n",
      "CH2 Unified-data-001404.csv: in 133382, out 133382\n",
      "CH1 Unified-data-001405.csv: in 256446, out 256446\n",
      "CH2 Unified-data-001405.csv: in 253944, out 253944\n",
      "CH1 Unified-data-001406.csv: in 261948, out 261948\n",
      "CH2 Unified-data-001406.csv: in 327192, out 327192\n",
      "CH1 Unified-data-001407.csv: in 311524, out 311524\n",
      "CH2 Unified-data-001407.csv: in 288476, out 288476\n",
      "CH1 Unified-data-001408.csv: in 295385, out 295385\n",
      "CH2 Unified-data-001408.csv: in 304615, out 304615\n",
      "CH1 Unified-data-001409.csv: in 319597, out 319597\n",
      "CH2 Unified-data-001409.csv: in 280403, out 280403\n",
      "CH1 Unified-data-001410.csv: in 260885, out 260885\n",
      "CH2 Unified-data-001410.csv: in 339115, out 339115\n"
     ]
    }
   ],
   "source": [
    "RunSettings()\n",
    "\n",
    "if settings['GenerateDatabase']:\n",
    "    print('Generating database')\n",
    "    GenerateDatabase(settings['Verbose'])\n",
    "   \n",
    "if settings['PhaseAnalysis']:\n",
    "    print('Running phase analysis')\n",
    "    PhaseAnalysis(settings['Verbose'], settings['ShowPhasePlots'])\n",
    "\n",
    "if settings['GeneratePolarPlots']:\n",
    "    print('Generating polar plots')\n",
    "    PolarPlots(settings['Verbose'], settings['ShowPolarPlots'])\n",
    "    \n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
